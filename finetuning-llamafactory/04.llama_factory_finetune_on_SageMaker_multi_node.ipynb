{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker - Multi Nodes\n",
    "# 4. 使用SageMaker Training Job分布式训练\n",
    "- 本示例使用deepspeed进行多机多GPU卡分布式训练\n",
    "- 如果指定instance数量为1，则是单机多GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Pre-requisites 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "### 数据集1. 从huggingface上下载ruozhiba数据集\n",
    "- 该数据集有近5k条数据，本次实验我们可以只用前1k条做训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4898\n",
      "\n",
      "Training sample:\n",
      "\n",
      "{'input': '', 'instruction': '大家有没有发现，无论考试还是啥，每次都是第一后面接着就是第二，第三名为什么从来就没有拿第二名或者第一名过，一定是有黑幕', 'output': '这个问题可能是一个误解。每次考试或比赛的排名是根据成绩或表现依次排列的，第一名之后自然是第二名，第三名之后紧接着是第四名。这是一个简单的逻辑顺序，没有什么黑幕。至于某个人能否拿不同的名次，取决于他的表现和成绩。如果一个人一直在某个名次上下波动，这是正常的现象。每次考试或比赛都有不同的参与者和变数，结果可能会不同。所以，第三名的人没有拿过第二名或者第一名，只是因为在那次特定的情况下他的表现决定了名次，而非有黑幕的存在。'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "dataset_name = \"hfl/ruozhiba_gpt4\"\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\",revision='41d2c61beb86c8d4c61916cc656c39d018c40ce5')\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'output': '这个问题是针对《海绵宝宝》中的蟹老板和他的快餐店“蟹堡王”的情节。在动画中，蟹黄堡是蟹堡王里最受欢迎的食品，但是具体配方是保密的。虽然蟹老板是一只蟹，但是该动画并未明确指出蟹黄堡是使用蟹肉制作的。事实上，这更多是一种戏剧化的设定，用来增加情节的幽默感。因此，从字面上来理解蟹老板制作蟹黄堡吃掉同类是不正确的，这只是一种讽刺的艺术表现手法，旨在娱乐观众而已。', 'input': '', 'instruction': '为什么蟹老板做蟹黄堡蚕食它的同类？'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集2. 身份数据集\n",
    "```json\n",
    "[{'instruction': 'hi',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'hello',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'Who are you?',\n",
    "  'input': '',\n",
    "  'output': 'I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'}]\n",
    "```\n",
    "把其中的name和author替换成您自己想替换的值，这样微调完成之后，问模型“你是谁，谁创造的你？”这类的身份问题，模型就会按这个新的值来回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_identity(origin_obj,name,author):\n",
    "    ret = []\n",
    "    for ele in origin_obj:\n",
    "        ele['output'] = ele['output'].replace(\"{{name}}\",name).replace(\"{{author}}\",author)\n",
    "        ret.append(ele)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 替换成您自己的设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = 'RiverBot'\n",
    "AUTHOR = 'GOGOGO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'hi',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am RiverBot, an AI assistant developed by River. How can I assist you today?'},\n",
       " {'instruction': 'hello',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am RiverBot, an AI assistant developed by River. How can I assist you today?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/identity.json'\n",
    "with open(file_name) as f:\n",
    "    identity = json.load(f)\n",
    "\n",
    "identity_2 = format_identity(identity,name='RiverBot',author='River')\n",
    "identity_2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./train',exist_ok=True)\n",
    "with open('./train/identity_2.json','w') as f:\n",
    "    json.dump(identity_2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据copy至S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_data_uri = f\"s3://{default_bucket}/dataset-for-training\"\n",
    "training_input_path = f'{s3_data_uri}/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c3df1c5ecb42a9b586bc72f088c251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset to: s3://sagemaker-us-east-1-342367142984/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "train_dataset.to_json('./train/ruozhiba.json')\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/ruozhiba.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/identity_2.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/dataset_info.json'\n",
    "with open(file_name) as f:\n",
    "    datainfo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['identity']={'file_name': 'identity_2.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['ruozhiba']={\n",
    "    'file_name':'ruozhiba.json',\n",
    "    \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "  }      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./LLaMA-Factory/data/dataset_info.json','w') as f:\n",
    "    json.dump(fp=f,obj=datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 训练配置yaml文件\n",
    "### 从LLaMA-Factory/examples/train_lora/目录中复制出llama3_lora_sft_ds3.yaml，并修改\n",
    "- 本次实验是使用Lora训练\n",
    "- 如果用全量微调，则使用LLaMA-Factory/examples/train_lora/llama3_lora_sft_ds3.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'deepspeed': 'examples/deepspeed/ds_z3_config.json',\n",
       " 'dataset': 'identity,alpaca_en_demo',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 1024,\n",
       " 'max_samples': 1000,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': 'saves/llama3-8b/lora/sft',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load template\n",
    "import yaml\n",
    "file_name = './LLaMA-Factory/examples/train_lora/llama3_lora_sft_ds3.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本次实验我们使用原始精度的LLaMA-3-8b， 从hf的repo 'unsloth/llama-3-8b-Instruct' 下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'unsloth/llama-3-8b-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'unsloth/llama-3-8b-Instruct',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'deepspeed': 'examples/deepspeed/ds_z3_config.json',\n",
       " 'dataset': 'identity,ruozhiba',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 2048,\n",
       " 'max_samples': 200,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': '/tmp/finetuned_model',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 2,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500,\n",
       " 'warmup_steps': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "doc['model_name_or_path'] = model_id\n",
    "doc['output_dir'] ='/tmp/finetuned_model'\n",
    "doc['num_train_epochs'] = 3\n",
    "doc['warmup_steps'] = 10\n",
    "doc['per_device_train_batch_size'] =1\n",
    "doc['gradient_accumulation_steps'] =2\n",
    "# doc['lora_target'] = 'all'\n",
    "doc['cutoff_len'] = 2048\n",
    "#实验时间，只选取前1000条数据做训练\n",
    "doc['max_samples'] = 200\n",
    "doc['dataset'] = 'identity,ruozhiba'\n",
    "doc['eval_steps'] = 500\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sg_config = 'sg_config_multl_node_lora_ds.yaml'\n",
    "with open(f'./LLaMA-Factory/{sg_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置Lora权重Merge 配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = './LLaMA-Factory/examples/merge_lora/llama3_lora_sft.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc2 = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sg_lora_merge_config = 'sg_config_lora_merge.yaml'\n",
    "doc2['model_name_or_path'] = model_id\n",
    "doc2['adapter_name_or_path'] ='/tmp/finetuned_model'\n",
    "doc2['export_dir'] ='/tmp/finetuned_model_merged'\n",
    "with open(f'./LLaMA-Factory/{sg_lora_merge_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交训练任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型输出s3目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "destination_s3 = f's3://{default_bucket}/llama3-8b-lora-sft-ds/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240710163701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-8b-instruct-finetune-2024-07-10-16-37-01-740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 16:37:05 Starting - Starting the training job...\n",
      "2024-07-10 16:37:06 Pending - Training job waiting for capacity......................................"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "instance_count = 2\n",
    "instance_type = 'ml.g5.48xlarge' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-instruct-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"merge_lora\":\"1\", ##是否合并lora模型\n",
    "    \"sg_config\":sg_config,\n",
    "    \"sg_lora_merge_config\":sg_lora_merge_config,\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    'OUTPUT_MODEL_S3_PATH': destination_s3\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry-multi-nodes.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)\n",
    "\n",
    "# # data in channel will be automatically copied to each node - /opt/ml/input/data/train1\n",
    "#input_channel = {'train': f's3://{sagemaker_default_bucket}/datasets/qiandao/{version}/train.json'}\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
