{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker \n",
    "# 1. Single GPU QLORA- 本地notebook实例训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker boto3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install torch==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集1. 从huggingface上下载ruozhiba数据集\n",
    "- 改数据集有近5k条数据，本次实验我们可以只用前1k条做训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4898\n",
      "\n",
      "Training sample:\n",
      "\n",
      "{'input': '', 'instruction': '最近怎么了，莫名其妙的想说脏话 可是我他妈是一个哑巴啊！', 'output': '情绪的表达并不总是需要通过语言，即使是哑巴，也有可能感受到想要表达强烈情绪或压力的冲动。这种想说脏话的冲动可能是由于内部情绪压力或者对某些情况的强烈反应。即便不能通过说话来表达，可能会通过手势、面部表情或其他非语言方式来表达自己的情绪。建议寻找适合自己的方式来管理和表达这些情绪，比如使用写作、艺术创作或与理解你的朋友通过其他方式沟通。'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "dataset_name = \"hfl/ruozhiba_gpt4\"\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\",revision='41d2c61beb86c8d4c61916cc656c39d018c40ce5')\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集2. 身份数据集\n",
    "```json\n",
    "[{'instruction': 'hi',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'hello',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'Who are you?',\n",
    "  'input': '',\n",
    "  'output': 'I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'}]\n",
    "```\n",
    "把其中的name和author替换成您自己想替换的值，这样微调完成之后，问模型“你是谁，谁创造的你？”这类的身份问题，模型就会按这个新的值来回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_identity(origin_obj,name,author):\n",
    "    ret = []\n",
    "    for ele in origin_obj:\n",
    "        ele['output'] = ele['output'].replace(\"{{name}}\",name).replace(\"{{author}}\",author)\n",
    "        ret.append(ele)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 替换成您自己的设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = 'NANE'\n",
    "AUTHOR = 'CK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/finetuning-on-aws/finetuning-llamafactory'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/finetuning-on-aws/finetuning-llamafactory\n",
      "/home/ec2-user/SageMaker/finetuning-on-aws/finetuning-llamafactory\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd ~/SageMaker/finetuning-on-aws/finetuning-llamafactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/finetuning-on-aws/finetuning-llamafactory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'hi',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am NANE, an AI assistant developed by CK. How can I assist you today?'},\n",
       " {'instruction': 'hello',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am NANE, an AI assistant developed by CK. How can I assist you today?'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/identity.json'\n",
    "with open(file_name) as f:\n",
    "    identity = json.load(f)\n",
    "identity_2 = format_identity(identity,name=NAME,author=AUTHOR)\n",
    "identity_2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./train',exist_ok=True)\n",
    "with open('./train/identity_2.json','w') as f:\n",
    "    json.dump(identity_2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据copy至S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_data_uri = f\"s3://{default_bucket}/dataset-for-training\"\n",
    "training_input_path = f'{s3_data_uri}/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-342367142984/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "print(training_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313bde1eb21646bdbf1319961d79e27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset to: s3://sagemaker-us-west-2-342367142984/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "train_dataset.to_json('./train/ruozhiba.json')\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/ruozhiba.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/identity_2.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = './LLaMA-Factory/data/dataset_info.json'\n",
    "with open(file_name) as f:\n",
    "    datainfo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['identity']={'file_name': 'identity_2.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['ruozhiba']={\n",
    "    'file_name':'ruozhiba.json',\n",
    "    \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "  }      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./LLaMA-Factory/data/dataset_info.json','w') as f:\n",
    "    json.dump(fp=f,obj=datainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 训练配置yaml文件\n",
    "###  从LLaMA-Factory/examples/train_qlora/目录中复制出llama3_lora_sft_awq.yaml，并修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'dataset': 'identity,alpaca_en_demo',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 1024,\n",
       " 'max_samples': 1000,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': 'saves/llama3-8b/lora/sft',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'bf16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load template\n",
    "import yaml\n",
    "file_name = './LLaMA-Factory/examples/train_qlora/llama3_lora_sft_awq.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#设置模型的保存目录在本notebook实例本地\n",
    "save_dir = '/home/ec2-user/SageMaker/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory/finetuned_model'\n",
    "# doc['output_dir'] = save_dir\n",
    "\n",
    "# 如果是用SageMaker则使用以下模型文件路径\n",
    "doc['output_dir'] ='/tmp/finetuned_model'\n",
    "doc['per_device_train_batch_size'] =1\n",
    "doc['gradient_accumulation_steps'] =8\n",
    "# doc['lora_target'] = 'all'\n",
    "doc['cutoff_len'] = 2048\n",
    "doc['num_train_epochs'] = 5.0\n",
    "doc['warmup_steps'] = 10\n",
    "\n",
    "#实验时间，只选取前200条数据做训练\n",
    "doc['max_samples'] = 200 \n",
    "#数据集\n",
    "doc['dataset'] = 'identity,ruozhiba'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存为训练配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'dataset': 'identity,ruozhiba',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 2048,\n",
       " 'max_samples': 200,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': '/tmp/finetuned_model',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 5.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'bf16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500,\n",
       " 'warmup_steps': 10}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_config = 'sg_config_qlora.yaml'\n",
    "with open(f'./LLaMA-Factory/{sg_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc, f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地GPU测试提交 Training job\n",
    "\n",
    "### 由于我们的实验环境限制，无法提交Training Job，所以在本次实验是在notebook实例中进行训练\n",
    "### 如果您在自己的AWS环境中，且有SageMaker Training Job 所需GPU实例的quota，则可以用如下代码提交，instance_type改成'ml.g5.2xlarge' \n",
    "\n",
    "```python\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "instance_count = 1\n",
    "instance_type = 'local_gpu' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-qlora-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    \"sg_config\":sg_config,\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{default_bucket}/llama3-8b-qlora/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry_single_lora.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240829190926\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "#使用本地机器，也可以指定为 ml.g5.2xlarge等其他实例\n",
    "instance_type = 'local_gpu' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-qlora-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    \"sg_config\":sg_config,\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{default_bucket}/llama3-8b-qlora/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry_single_lora.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-8b-qlora-finetune-2024-08-29-19-09-43-243\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.local.image:'Docker Compose' is not installed. Proceeding to check for 'docker-compose' CLI.\n",
      "INFO:sagemaker.local.image:'Docker Compose' found using Docker Compose CLI.\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-ci1oq:\n",
      "    command: train\n",
      "    container_name: kqh8jyxm6x-algo-1-ci1oq\n",
      "    deploy:\n",
      "      resources:\n",
      "        reservations:\n",
      "          devices:\n",
      "          - capabilities:\n",
      "            - gpu\n",
      "            count: all\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.2.0-gpu-py310\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-ci1oq\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpq2u9bdu0/algo-1-ci1oq/output:/opt/ml/output\n",
      "    - /tmp/tmpq2u9bdu0/algo-1-ci1oq/input:/opt/ml/input\n",
      "    - /tmp/tmpq2u9bdu0/algo-1-ci1oq/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpq2u9bdu0/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "version: '2.3'\n",
      "\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker pull 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.2.0-gpu-py310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.image:image pulled: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:2.2.0-gpu-py310\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpq2u9bdu0/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container kqh8jyxm6x-algo-1-ci1oq  Creating\n",
      " Container kqh8jyxm6x-algo-1-ci1oq  Created\n",
      "Attaching to kqh8jyxm6x-algo-1-ci1oq\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:39,776 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:39,797 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:39,806 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:39,809 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:39,812 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:40,934 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:13:41,625 sagemaker-training-toolkit INFO     Installing module with the following command:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | /opt/conda/bin/python3.10 -m pip install . -r requirements.txt\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Processing /opt/ml/code\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-eqcj_p4h/unsloth_f10ae898e1534d78bdc61ead624c89ba\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-eqcj_p4h/unsloth_f10ae898e1534d78bdc61ead624c89ba\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Resolved https://github.com/unslothai/unsloth.git to commit 976d11a10d54383aeb7a692c69e01151a20bfd72\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting transformers>=4.41.2 (from -r requirements.txt (line 1))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 2.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting datasets>=2.16.0 (from -r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting accelerate>=0.30.1 (from -r requirements.txt (line 3))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting peft>=0.11.1 (from -r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting trl>=0.8.6 (from -r requirements.txt (line 5))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading trl-0.10.1-py3-none-any.whl.metadata (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting gradio>=4.0.0 (from -r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading gradio-4.42.0-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting sentencepiece (from -r requirements.txt (line 10))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tiktoken (from -r requirements.txt (line 11))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.20.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting uvicorn (from -r requirements.txt (line 13))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.7.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting fastapi (from -r requirements.txt (line 15))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading fastapi-0.112.2-py3-none-any.whl.metadata (27 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting sse-starlette (from -r requirements.txt (line 16))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.8.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting fire (from -r requirements.txt (line 18))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 11.4 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (23.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting deepspeed==0.14.4 (from -r requirements.txt (line 22))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading deepspeed-0.14.4.tar.gz (1.3 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting autoawq==0.2.5 (from -r requirements.txt (line 23))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting metrics (from -r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.3.3.tar.gz (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting bitsandbytes (from -r requirements.txt (line 25))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting rouge-chinese (from -r requirements.txt (line 26))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting jieba (from -r requirements.txt (line 27))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 91.4 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting peft>=0.11.1 (from -r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting flash_attn==2.6.1 (from -r requirements.txt (line 29))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading flash_attn-2.6.1.tar.gz (2.6 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 115.0 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (5.9.8)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (2.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (4.66.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting safetensors (from peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting huggingface-hub>=0.17.0 (from peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting hjson (from deepspeed==0.14.4->-r requirements.txt (line 22))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.4->-r requirements.txt (line 22)) (1.11.1.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-ml-py (from deepspeed==0.14.4->-r requirements.txt (line 22))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting py-cpuinfo (from deepspeed==0.14.4->-r requirements.txt (line 22))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tokenizers>=0.12.1 (from autoawq==0.2.5->-r requirements.txt (line 23))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (4.11.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (0.22.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting autoawq-kernels (from autoawq==0.2.5->-r requirements.txt (line 23))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading autoawq_kernels-0.0.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting regex!=2019.12.17 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 9.5 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tokenizers>=0.12.1 (from autoawq==0.2.5->-r requirements.txt (line 23))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting xxhash (from datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.5.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting aiohttp (from datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tyro-0.8.10-py3-none-any.whl.metadata (8.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting anyio<5.0,>=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting gradio-client==1.3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading importlib_resources-6.4.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 11.0 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting urllib3~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 13))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.18.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting starlette<0.39.0,>=0.37.2 (from fastapi->-r requirements.txt (line 15))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading starlette-0.38.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.52.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (1.16.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting termcolor (from fire->-r requirements.txt (line 18))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting Pygments==2.2.0 (from metrics->-r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting pathspec==0.5.5 (from metrics->-r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading pathspec-0.5.5.tar.gz (21 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting pathlib2>=2.3.0 (from metrics->-r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (3.7)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.7.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (1.12)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (3.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting torch>=1.13.0 (from peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting triton==2.3.1 (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 218.2/218.2 MB 7.8 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31)) (0.42.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting hf-transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | INFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.8.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | INFO: pip is still looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting markdown-it-py<3.0.0,>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.5-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.4-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting commonmark<0.10.0,>=0.9.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.0.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-13.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.4.4-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.4.3-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-12.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-11.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.6)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-11.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tyro-0.8.9-py3-none-any.whl.metadata (8.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-11.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.16.2-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.16.1-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.16.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.15.2-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.15.1-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.15.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.14.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.13.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rich-10.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.4-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.2-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.1-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting typer-slim==0.12.0 (from typer-slim[standard]==0.12.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer_slim-0.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting typer-cli==0.12.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer_cli-0.12.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting metrics (from -r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.3.2.tar.gz (18 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.3.1.tar.gz (14 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.3.0.tar.gz (14 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.2.8.tar.gz (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting pathspec==0.5.3 (from metrics->-r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading pathspec-0.5.3.tar.gz (20 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting metrics (from -r requirements.txt (line 24))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.2.7.tar.gz (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading metrics-0.2.6.tar.gz (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 24)) (2.18.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (1.3.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting autoawq-kernels (from autoawq==0.2.5->-r requirements.txt (line 23))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 40.7 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 17.2 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/9.5 MB 105.5 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.3/527.3 kB 54.7 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.1/315.1 kB 46.2 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.1/280.1 kB 40.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading gradio-4.42.0-py3-none-any.whl (16.8 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 72.1 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.7/318.7 kB 53.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 90.2 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 96.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 13.0 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading fastapi-0.112.2-py3-none-any.whl (93 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.5/93.5 kB 22.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.5/137.5 MB 16.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 22.6 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiohttp-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 98.2 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 14.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.4/76.4 kB 10.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 19.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.5/417.5 kB 53.7 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading importlib_resources-6.4.4-py3-none-any.whl (35 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.9/141.9 kB 21.4 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.5/776.5 kB 67.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading ruff-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.3 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.3/10.3 MB 119.5 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.5/435.5 kB 72.0 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading starlette-0.38.2-py3-none-any.whl (72 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 14.4 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 97.1 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 kB 10.6 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading tyro-0.8.10-py3-none-any.whl (105 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.7/105.7 kB 24.0 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.4/121.4 kB 23.5 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl (33.4 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.4/33.4 MB 62.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 14.8 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 8.9 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 40.3 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiohappyeyeballs-2.4.0-py3-none-any.whl (12 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 48.7 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.3/124.3 kB 28.1 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 27.5 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.6/301.6 kB 51.6 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 127.1 MB/s eta 0:00:00\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheels for collected packages: deepspeed, flash_attn, llamafactory, fire, metrics, jieba, unsloth\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for deepspeed (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for deepspeed: filename=deepspeed-0.14.4-py3-none-any.whl size=1445521 sha256=e5c961e7fff73f44f2cd7788109f76803795a5ab26ccba03e2969f9937cd7e85\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /root/.cache/pip/wheels/8e/bc/a3/608e90bbb301848b78fd75d24d6d43ba3074de968fc0e397ac\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for flash_attn (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for flash_attn (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for flash_attn: filename=flash_attn-2.6.1-cp310-cp310-linux_x86_64.whl size=198377647 sha256=cf6e577925a9b80015a4f94a82a464baf13165dfd3c20289a11d14f8d181f274\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /root/.cache/pip/wheels/91/6a/38/f0faa036b4ac73a73247386f1ab1bb4cb4f6e72e6861a779f1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for llamafactory (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for llamafactory (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-py3-none-any.whl size=232200 sha256=5306d5fa6795c028164d4adaadedc4deecbe379887ecd732b33fa94b030fb856\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /tmp/pip-ephem-wheel-cache-0wh8pjcs/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for fire (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for fire (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=2a360a210817efce09d7dd1274deb29e07c61ba6f040fc019636daa4b1f2a56f\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for metrics (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for metrics (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for metrics: filename=metrics-0.2.6-py3-none-any.whl size=12795 sha256=32d546126afed31a2ae5f2a01e0d1ddade5300b9ebe405d9bfb5546855cb0bed\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /root/.cache/pip/wheels/c5/ca/d1/efae53ccccfba939be676f12c0449626e9da6ec1f6ec6e6e3d\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for jieba (setup.py): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for jieba (setup.py): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=2061025f0df635019a4ec1a98e7e7e89123211c0838eddb24c4aaeb896c740dc\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /root/.cache/pip/wheels/c9/69/31/d56d90b22a1777b0b231e234b00302a55be255930f8bd92dcd\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for unsloth (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheel for unsloth (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for unsloth: filename=unsloth-2024.8-py3-none-any.whl size=145817 sha256=1a59012d8f5162a10dabde3d7a4daa7a7cd3cb2786e84e9fca7dc8944bd4e938\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /tmp/pip-ephem-wheel-cache-0wh8pjcs/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully built deepspeed flash_attn llamafactory fire metrics jieba unsloth\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing collected packages: sentencepiece, pydub, py-cpuinfo, nvidia-ml-py, jieba, hjson, xxhash, websockets, urllib3, unsloth, tomlkit, termcolor, sniffio, shtab, semantic-version, safetensors, ruff, rouge-chinese, regex, python-multipart, orjson, multidict, metrics, importlib-resources, hf-transfer, h11, frozenlist, ffmpy, docstring-parser, async-timeout, aiohappyeyeballs, aiofiles, yarl, uvicorn, httpcore, fire, anyio, aiosignal, xformers, tyro, typer, tiktoken, starlette, huggingface-hub, httpx, flash_attn, deepspeed, bitsandbytes, autoawq-kernels, aiohttp, tokenizers, sse-starlette, gradio-client, fastapi, accelerate, transformers, gradio, datasets, trl, peft, autoawq, llamafactory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Attempting uninstall: urllib3\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Found existing installation: urllib3 1.26.19\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Uninstalling urllib3-1.26.19:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully uninstalled urllib3-1.26.19\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Attempting uninstall: typer\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Found existing installation: typer 0.9.4\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Uninstalling typer-0.9.4:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully uninstalled typer-0.9.4\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Attempting uninstall: flash_attn\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Found existing installation: flash-attn 2.0.4\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Uninstalling flash-attn-2.0.4:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully uninstalled flash-attn-2.0.4\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Attempting uninstall: accelerate\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Found existing installation: accelerate 0.22.0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Uninstalling accelerate-0.22.0:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully uninstalled accelerate-0.22.0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.5 which is incompatible.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | transformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.6.1 which is incompatible.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.5 which is incompatible.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully installed accelerate-0.33.0 aiofiles-23.2.1 aiohappyeyeballs-2.4.0 aiohttp-3.10.5 aiosignal-1.3.1 anyio-4.4.0 async-timeout-4.0.3 autoawq-0.2.5 autoawq-kernels-0.0.6 bitsandbytes-0.43.3 datasets-2.21.0 deepspeed-0.14.4 docstring-parser-0.16 fastapi-0.112.2 ffmpy-0.4.0 fire-0.6.0 flash_attn-2.6.1 frozenlist-1.4.1 gradio-4.42.0 gradio-client-1.3.0 h11-0.14.0 hf-transfer-0.1.8 hjson-3.1.0 httpcore-1.0.5 httpx-0.27.2 huggingface-hub-0.24.6 importlib-resources-6.4.4 jieba-0.42.1 llamafactory-0.8.4.dev0 metrics-0.2.6 multidict-6.0.5 nvidia-ml-py-12.560.30 orjson-3.10.7 peft-0.11.1 py-cpuinfo-9.0.0 pydub-0.25.1 python-multipart-0.0.9 regex-2024.7.24 rouge-chinese-1.0.3 ruff-0.6.3 safetensors-0.4.4 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.3 starlette-0.38.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 transformers-4.44.2 trl-0.10.1 typer-0.12.5 tyro-0.8.10 unsloth-2024.8 urllib3-2.2.2 uvicorn-0.30.6 websockets-12.0 xformers-0.0.24 xxhash-3.5.0 yarl-1.9.4\n",
      "kqh8jyxm6x-algo-1-ci1oq  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] A new release of pip is available: 24.0 -> 24.2\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] To update, run: pip install --upgrade pip\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,007 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,007 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,091 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,099 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,131 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,139 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,171 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,179 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,182 sagemaker-training-toolkit INFO     Invoking user script\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Training Env:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"additional_framework_parameters\": {},\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"channel_input_dirs\": {},\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"current_host\": \"algo-1-ci1oq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"current_instance_group\": \"homogeneousCluster\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"current_instance_group_hosts\": [],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"current_instance_type\": \"local\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"distribution_hosts\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |         \"algo-1-ci1oq\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"distribution_instance_groups\": [],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"hosts\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |         \"algo-1-ci1oq\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"hyperparameters\": {},\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"input_data_config\": {},\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"input_dir\": \"/opt/ml/input\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"instance_groups\": [],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"instance_groups_dict\": {},\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"is_hetero\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"is_master\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"is_modelparallel_enabled\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"is_smddpmprun_installed\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"is_smddprun_installed\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"job_name\": \"llama3-8b-qlora-finetune-2024-08-29-19-09-43-243\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"log_level\": 20,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"master_hostname\": \"algo-1-ci1oq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"model_dir\": \"/opt/ml/model\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"module_dir\": \"s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora-finetune-2024-08-29-19-09-43-243/source/sourcedir.tar.gz\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"module_name\": \"entry_single_lora\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"network_interface_name\": \"eth0\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"num_cpus\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"num_gpus\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"num_neurons\": 0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"output_dir\": \"/opt/ml/output\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"resource_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |         \"current_host\": \"algo-1-ci1oq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |         \"hosts\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |             \"algo-1-ci1oq\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |         ]\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"user_entry_point\": \"entry_single_lora.py\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Environment variables:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_HOSTS=[\"algo-1-ci1oq\"]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_HPS={}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_USER_ENTRY_POINT=entry_single_lora.py\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_FRAMEWORK_PARAMS={}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-ci1oq\",\"hosts\":[\"algo-1-ci1oq\"]}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_INPUT_DATA_CONFIG={}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_CHANNELS=[]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_CURRENT_HOST=algo-1-ci1oq\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_CURRENT_INSTANCE_TYPE=local\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_INSTANCE_GROUPS=[]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_INSTANCE_GROUPS_DICT={}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_IS_HETERO=false\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_MODULE_NAME=entry_single_lora\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_LOG_LEVEL=20\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_INPUT_DIR=/opt/ml/input\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_NUM_CPUS=8\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_NUM_GPUS=1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_NUM_NEURONS=0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_MODEL_DIR=/opt/ml/model\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_MODULE_DIR=s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora-finetune-2024-08-29-19-09-43-243/source/sourcedir.tar.gz\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-ci1oq\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-ci1oq\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-ci1oq\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama3-8b-qlora-finetune-2024-08-29-19-09-43-243\",\"log_level\":20,\"master_hostname\":\"algo-1-ci1oq\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora-finetune-2024-08-29-19-09-43-243/source/sourcedir.tar.gz\",\"module_name\":\"entry_single_lora\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-ci1oq\",\"hosts\":[\"algo-1-ci1oq\"]},\"user_entry_point\":\"entry_single_lora.py\"}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_USER_ARGS=[]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "kqh8jyxm6x-algo-1-ci1oq  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Invoking script with the following command:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | /opt/conda/bin/python3.10 -m entry_single_lora\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,184 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:15:07,184 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Obtaining file:///opt/ml/code\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Checking if build backend supports build_editable: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Checking if build backend supports build_editable: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build editable: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build editable: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing editable metadata (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building wheels for collected packages: llamafactory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building editable for llamafactory (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Building editable for llamafactory (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=20816 sha256=06cb50edf02af16d4611f72b25be82bad27f331dc4350c83661713e4eeea1121\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Stored in directory: /tmp/pip-ephem-wheel-cache-ps74bz6x/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully built llamafactory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing collected packages: llamafactory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Attempting uninstall: llamafactory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Found existing installation: llamafactory 0.8.4.dev0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Uninstalling llamafactory-0.8.4.dev0:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully uninstalled llamafactory-0.8.4.dev0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Successfully installed llamafactory-0.8.4.dev0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] A new release of pip is available: 24.0 -> 24.2\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] To update, run: pip install --upgrade pip\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-kru4_5o2/unsloth_1da33424e77d4e408a63356e8d37adf0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-kru4_5o2/unsloth_1da33424e77d4e408a63356e8d37adf0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Resolved https://github.com/unslothai/unsloth.git to commit 976d11a10d54383aeb7a692c69e01151a20bfd72\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Installing build dependencies: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Getting requirements to build wheel: finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): started\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.44.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.21.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.33.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: peft>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.11.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: trl>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.10.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: gradio>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.42.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.7.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.20.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.30.6)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.7.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (0.112.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.1.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.8.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (0.6.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (23.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: deepspeed==0.14.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.14.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: autoawq==0.2.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.2.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: metrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.2.6)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (0.43.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: rouge-chinese in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (1.0.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.42.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: flash_attn==2.6.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (2.6.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (5.9.8)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (2.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (4.66.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (0.4.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft>=0.11.1->-r requirements.txt (line 4)) (0.24.6)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.4->-r requirements.txt (line 22)) (3.1.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.4->-r requirements.txt (line 22)) (1.11.1.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.4->-r requirements.txt (line 22)) (12.560.30)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.4->-r requirements.txt (line 22)) (9.0.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (0.19.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (4.11.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (0.22.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: autoawq-kernels in /opt/conda/lib/python3.10/site-packages (from autoawq==0.2.5->-r requirements.txt (line 23)) (0.0.6)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2024.7.24)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.5.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.5.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.10.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.8.6->-r requirements.txt (line 5)) (0.8.10)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (23.2.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.4.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: gradio-client==1.3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.10.7)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.6.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.2.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (12.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.18.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 15)) (0.38.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.52.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (1.16.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 18)) (2.4.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 24)) (2.18.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (3.7)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.7.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (1.12)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (3.3)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (0.16)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (1.7.1)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Collecting xformers@ https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31))\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Using cached https://download.pytorch.org/whl/cu121/xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: wheel>=0.42.0 in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31)) (0.42.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: hf-transfer in /opt/conda/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[cu121-torch220]@ git+https://github.com/unslothai/unsloth.git->-r requirements.txt (line 31)) (0.1.8)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft>=0.11.1->-r requirements.txt (line 4)) (1.3.0)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] A new release of pip is available: 24.0 -> 24.2\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [notice] To update, run: pip install --upgrade pip\n",
      "kqh8jyxm6x-algo-1-ci1oq  | CITATION.cff\n",
      "kqh8jyxm6x-algo-1-ci1oq  | LICENSE\n",
      "kqh8jyxm6x-algo-1-ci1oq  | MANIFEST.in\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Makefile\n",
      "kqh8jyxm6x-algo-1-ci1oq  | README.md\n",
      "kqh8jyxm6x-algo-1-ci1oq  | README_zh.md\n",
      "kqh8jyxm6x-algo-1-ci1oq  | assets\n",
      "kqh8jyxm6x-algo-1-ci1oq  | build\n",
      "kqh8jyxm6x-algo-1-ci1oq  | data\n",
      "kqh8jyxm6x-algo-1-ci1oq  | docker\n",
      "kqh8jyxm6x-algo-1-ci1oq  | entry-multi-nodes.py\n",
      "kqh8jyxm6x-algo-1-ci1oq  | entry_single_lora.py\n",
      "kqh8jyxm6x-algo-1-ci1oq  | evaluation\n",
      "kqh8jyxm6x-algo-1-ci1oq  | examples\n",
      "kqh8jyxm6x-algo-1-ci1oq  | pyproject.toml\n",
      "kqh8jyxm6x-algo-1-ci1oq  | requirements.txt\n",
      "kqh8jyxm6x-algo-1-ci1oq  | s5cmd\n",
      "kqh8jyxm6x-algo-1-ci1oq  | scripts\n",
      "kqh8jyxm6x-algo-1-ci1oq  | setup.py\n",
      "kqh8jyxm6x-algo-1-ci1oq  | sg_config_qlora.yaml\n",
      "kqh8jyxm6x-algo-1-ci1oq  | src\n",
      "kqh8jyxm6x-algo-1-ci1oq  | tests\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp s3://sagemaker-us-west-2-342367142984/dataset-for-training/train/identity_2.json /opt/ml/code/data/identity_2.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp s3://sagemaker-us-west-2-342367142984/dataset-for-training/train/ruozhiba.json /opt/ml/code/data/ruozhiba.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [2024-08-29 19:15:38,541] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | df: /root/.triton/autotune: No such file or directory\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:41 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file added_tokens.json from cache at None\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file added_tokens.json from cache at None\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2269] 2024-08-29 19:15:42,051 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2513] 2024-08-29 19:15:42,341 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2513] 2024-08-29 19:15:42,341 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:42 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:42 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:42 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Generating train split: 0 examples [00:00, ? examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Generating train split: 91 examples [00:00, 8648.83 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16):  40%|███▉      | 36/91 [00:00<00:00, 337.85 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 426.89 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:43 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Generating train split: 0 examples [00:00, ? examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Generating train split: 4898 examples [00:00, 300579.41 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16):  39%|███▉      | 78/200 [00:00<00:00, 771.19 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Converting format of dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 977.89 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):   0%|          | 0/291 [00:00<?, ? examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):   7%|▋         | 19/291 [00:00<00:08, 32.32 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  13%|█▎        | 38/291 [00:00<00:04, 51.81 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  20%|█▉        | 57/291 [00:01<00:03, 64.50 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  26%|██▌       | 75/291 [00:01<00:03, 71.22 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  32%|███▏      | 93/291 [00:01<00:02, 75.26 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  38%|███▊      | 111/291 [00:01<00:02, 77.87 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  44%|████▍     | 129/291 [00:01<00:02, 79.72 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  51%|█████     | 147/291 [00:02<00:01, 81.37 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 165/291 [00:02<00:01, 82.60 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 183/291 [00:02<00:01, 82.86 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 201/291 [00:02<00:01, 83.86 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 219/291 [00:02<00:00, 83.92 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 237/291 [00:03<00:00, 84.45 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 255/291 [00:03<00:00, 85.04 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 273/291 [00:03<00:00, 90.24 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16): 100%|██████████| 291/291 [00:03<00:00, 94.98 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Running tokenizer on dataset (num_proc=16): 100%|██████████| 291/291 [00:03<00:00, 77.54 examples/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | training example:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | input_ids:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 452, 28919, 11, 459, 15592, 18328, 8040, 555, 31166, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | inputs:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Hello! I am NANE, an AI assistant developed by CK. How can I assist you today?<|eot_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | label_ids:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 452, 28919, 11, 459, 15592, 18328, 8040, 555, 31166, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | labels:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Hello! I am NANE, an AI assistant developed by CK. How can I assist you today?<|eot_id|>\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:15:48,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:15:48,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:15:48,323 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:15:48,323 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:15:48 - INFO - llamafactory.model.model_utils.quantization - Loading 4-bit AWQ-quantized model.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [WARNING|quantizer_awq.py:74] 2024-08-29 19:15:48,444 >> We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [WARNING|quantizer_awq.py:74] 2024-08-29 19:15:48,444 >> We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:3678] 2024-08-29 19:15:48,724 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/model.safetensors.index.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:3678] 2024-08-29 19:15:48,724 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/model.safetensors.index.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shards:  50%|█████     | 1/2 [01:50<01:50, 110.09s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shards: 100%|██████████| 2/2 [02:14<00:00, 59.70s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Downloading shards: 100%|██████████| 2/2 [02:14<00:00, 67.26s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:1606] 2024-08-29 19:18:03,242 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:1606] 2024-08-29 19:18:03,242 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:1038] 2024-08-29 19:18:03,243 >> Generate config GenerationConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:1038] 2024-08-29 19:18:03,243 >> Generate config GenerationConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.71s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.05s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:4507] 2024-08-29 19:18:06,025 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:4507] 2024-08-29 19:18:06,025 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:4515] 2024-08-29 19:18:06,025 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TechxGenus/Meta-Llama-3-8B-Instruct-AWQ.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modeling_utils.py:4515] 2024-08-29 19:18:06,025 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TechxGenus/Meta-Llama-3-8B-Instruct-AWQ.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:993] 2024-08-29 19:18:06,290 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/generation_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:993] 2024-08-29 19:18:06,290 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/generation_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:1038] 2024-08-29 19:18:06,290 >> Generate config GenerationConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"do_sample\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     128009\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:1038] 2024-08-29 19:18:06,290 >> Generate config GenerationConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"do_sample\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     128009\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,down_proj,o_proj,v_proj,q_proj,up_proj,gate_proj\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:18:06 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 1,071,910,912 || trainable%: 1.9565\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:648] 2024-08-29 19:18:06,884 >> Using auto half precision backend\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:648] 2024-08-29 19:18:06,884 >> Using auto half precision backend\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2134] 2024-08-29 19:18:07,211 >> ***** Running training *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2135] 2024-08-29 19:18:07,211 >>   Num examples = 261\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2134] 2024-08-29 19:18:07,211 >> ***** Running training *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2135] 2024-08-29 19:18:07,211 >>   Num examples = 261\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2136] 2024-08-29 19:18:07,211 >>   Num Epochs = 5\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2136] 2024-08-29 19:18:07,211 >>   Num Epochs = 5\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2137] 2024-08-29 19:18:07,211 >>   Instantaneous batch size per device = 1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2140] 2024-08-29 19:18:07,211 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2141] 2024-08-29 19:18:07,211 >>   Gradient Accumulation steps = 8\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2137] 2024-08-29 19:18:07,211 >>   Instantaneous batch size per device = 1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2140] 2024-08-29 19:18:07,211 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2141] 2024-08-29 19:18:07,211 >>   Gradient Accumulation steps = 8\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2142] 2024-08-29 19:18:07,211 >>   Total optimization steps = 160\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2142] 2024-08-29 19:18:07,211 >>   Total optimization steps = 160\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2143] 2024-08-29 19:18:07,217 >>   Number of trainable parameters = 20,971,520\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2143] 2024-08-29 19:18:07,217 >>   Number of trainable parameters = 20,971,520\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 0%|          | 0/160 [00:00<?, ?it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 1%|          | 1/160 [00:06<17:21,  6.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 1%|▏         | 2/160 [00:12<15:46,  5.99s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2%|▏         | 3/160 [00:17<15:08,  5.79s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2%|▎         | 4/160 [00:23<14:47,  5.69s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 3%|▎         | 5/160 [00:29<15:00,  5.81s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 4%|▍         | 6/160 [00:35<14:56,  5.82s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 4%|▍         | 7/160 [00:40<14:32,  5.70s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 5%|▌         | 8/160 [00:46<14:17,  5.64s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 6%|▌         | 9/160 [00:50<13:31,  5.38s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 6%|▋         | 10/160 [00:56<13:41,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 2.47, 'grad_norm': 1.585640549659729, 'learning_rate': 0.0001, 'epoch': 0.31}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 6%|▋         | 10/160 [00:56<13:41,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 7%|▋         | 11/160 [01:02<13:42,  5.52s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 8%|▊         | 12/160 [01:07<13:38,  5.53s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 8%|▊         | 13/160 [01:13<13:23,  5.47s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 9%|▉         | 14/160 [01:18<13:13,  5.44s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 9%|▉         | 15/160 [01:23<13:03,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 10%|█         | 16/160 [01:29<13:07,  5.47s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 11%|█         | 17/160 [01:34<12:58,  5.44s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 11%|█▏        | 18/160 [01:40<13:01,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 12%|█▏        | 19/160 [01:46<13:03,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 12%|█▎        | 20/160 [01:51<12:56,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.8893, 'grad_norm': 1.7428035736083984, 'learning_rate': 9.890738003669029e-05, 'epoch': 0.61}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 12%|█▎        | 20/160 [01:51<12:56,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 13%|█▎        | 21/160 [01:56<12:38,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 14%|█▍        | 22/160 [02:02<12:25,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 14%|█▍        | 23/160 [02:07<12:21,  5.42s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 15%|█▌        | 24/160 [02:13<12:27,  5.49s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 16%|█▌        | 25/160 [02:18<12:18,  5.47s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 16%|█▋        | 26/160 [02:24<12:08,  5.44s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 17%|█▋        | 27/160 [02:29<12:06,  5.46s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 18%|█▊        | 28/160 [02:35<12:06,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 18%|█▊        | 29/160 [02:40<11:57,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 19%|█▉        | 30/160 [02:46<12:10,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.6381, 'grad_norm': 1.7987236976623535, 'learning_rate': 9.567727288213005e-05, 'epoch': 0.92}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 19%|█▉        | 30/160 [02:46<12:10,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 19%|█▉        | 31/160 [02:52<12:06,  5.63s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 20%|██        | 32/160 [02:57<11:46,  5.52s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 21%|██        | 33/160 [03:03<11:47,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 21%|██▏       | 34/160 [03:08<11:30,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 22%|██▏       | 35/160 [03:13<11:13,  5.39s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 22%|██▎       | 36/160 [03:18<10:59,  5.32s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 23%|██▎       | 37/160 [03:24<11:14,  5.49s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 24%|██▍       | 38/160 [03:30<11:17,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 24%|██▍       | 39/160 [03:35<11:06,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 25%|██▌       | 40/160 [03:41<10:59,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.464, 'grad_norm': 1.9165008068084717, 'learning_rate': 9.045084971874738e-05, 'epoch': 1.23}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 25%|██▌       | 40/160 [03:41<10:59,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 26%|██▌       | 41/160 [03:46<11:04,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 26%|██▋       | 42/160 [03:52<10:53,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 27%|██▋       | 43/160 [03:57<10:47,  5.53s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 28%|██▊       | 44/160 [04:03<10:35,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 28%|██▊       | 45/160 [04:08<10:36,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 29%|██▉       | 46/160 [04:14<10:36,  5.58s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 29%|██▉       | 47/160 [04:20<10:36,  5.63s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 30%|███       | 48/160 [04:25<10:28,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 31%|███       | 49/160 [04:31<10:21,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 31%|███▏      | 50/160 [04:37<10:16,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.3786, 'grad_norm': 1.943231463432312, 'learning_rate': 8.345653031794292e-05, 'epoch': 1.53}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 31%|███▏      | 50/160 [04:37<10:16,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 32%|███▏      | 51/160 [04:42<10:18,  5.68s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 32%|███▎      | 52/160 [04:48<09:56,  5.53s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 33%|███▎      | 53/160 [04:53<09:52,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 34%|███▍      | 54/160 [04:59<09:44,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 34%|███▍      | 55/160 [05:04<09:37,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 35%|███▌      | 56/160 [05:10<09:37,  5.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 36%|███▌      | 57/160 [05:15<09:34,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 36%|███▋      | 58/160 [05:21<09:20,  5.49s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 37%|███▋      | 59/160 [05:26<09:22,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 38%|███▊      | 60/160 [05:32<09:10,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.3986, 'grad_norm': 2.200094223022461, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.84}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 38%|███▊      | 60/160 [05:32<09:10,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 38%|███▊      | 61/160 [05:38<09:12,  5.58s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 39%|███▉      | 62/160 [05:43<08:59,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 39%|███▉      | 63/160 [05:49<08:58,  5.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 40%|████      | 64/160 [05:54<08:43,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 41%|████      | 65/160 [06:00<08:48,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 41%|████▏     | 66/160 [06:05<08:49,  5.63s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 42%|████▏     | 67/160 [06:11<08:39,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 42%|████▎     | 68/160 [06:16<08:27,  5.52s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 43%|████▎     | 69/160 [06:22<08:18,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 44%|████▍     | 70/160 [06:27<08:15,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.2546, 'grad_norm': 1.795565128326416, 'learning_rate': 6.545084971874738e-05, 'epoch': 2.15}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 44%|████▍     | 70/160 [06:27<08:15,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 44%|████▍     | 71/160 [06:33<08:23,  5.66s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 45%|████▌     | 72/160 [06:39<08:09,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 46%|████▌     | 73/160 [06:44<07:59,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 46%|████▋     | 74/160 [06:49<07:54,  5.52s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 47%|████▋     | 75/160 [06:55<07:53,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 48%|████▊     | 76/160 [07:01<07:46,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 48%|████▊     | 77/160 [07:06<07:46,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 49%|████▉     | 78/160 [07:12<07:38,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 49%|████▉     | 79/160 [07:17<07:29,  5.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 50%|█████     | 80/160 [07:23<07:23,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.1453, 'grad_norm': 2.3905630111694336, 'learning_rate': 5.522642316338268e-05, 'epoch': 2.45}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 50%|█████     | 80/160 [07:23<07:23,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 51%|█████     | 81/160 [07:29<07:21,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 51%|█████▏    | 82/160 [07:34<07:12,  5.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 52%|█████▏    | 83/160 [07:40<07:10,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 52%|█████▎    | 84/160 [07:45<07:06,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 53%|█████▎    | 85/160 [07:51<07:00,  5.60s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 54%|█████▍    | 86/160 [07:56<06:47,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 54%|█████▍    | 87/160 [08:02<06:35,  5.42s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 55%|█████▌    | 88/160 [08:07<06:27,  5.38s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 56%|█████▌    | 89/160 [08:12<06:27,  5.46s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 56%|█████▋    | 90/160 [08:18<06:23,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.1131, 'grad_norm': 2.427075147628784, 'learning_rate': 4.477357683661734e-05, 'epoch': 2.76}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 56%|█████▋    | 90/160 [08:18<06:23,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 57%|█████▋    | 91/160 [08:24<06:24,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 57%|█████▊    | 92/160 [08:29<06:20,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 58%|█████▊    | 93/160 [08:35<06:10,  5.52s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 59%|█████▉    | 94/160 [08:40<06:07,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 59%|█████▉    | 95/160 [08:46<06:02,  5.58s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 60%|██████    | 96/160 [08:52<05:54,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 61%|██████    | 97/160 [08:56<05:36,  5.35s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 61%|██████▏   | 98/160 [09:02<05:35,  5.42s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 62%|██████▏   | 99/160 [09:07<05:29,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 62%|██████▎   | 100/160 [09:13<05:28,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 1.0523, 'grad_norm': 2.080439329147339, 'learning_rate': 3.4549150281252636e-05, 'epoch': 3.07}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 62%|██████▎   | 100/160 [09:13<05:28,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 63%|██████▎   | 101/160 [09:19<05:35,  5.69s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 64%|██████▍   | 102/160 [09:25<05:31,  5.71s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 64%|██████▍   | 103/160 [09:30<05:19,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 65%|██████▌   | 104/160 [09:36<05:13,  5.60s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 66%|██████▌   | 105/160 [09:41<05:05,  5.55s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 66%|██████▋   | 106/160 [09:47<05:04,  5.64s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 67%|██████▋   | 107/160 [09:52<04:46,  5.41s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 68%|██████▊   | 108/160 [09:57<04:36,  5.33s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 68%|██████▊   | 109/160 [10:03<04:36,  5.43s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 69%|██████▉   | 110/160 [10:08<04:32,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.9092, 'grad_norm': 2.585332155227661, 'learning_rate': 2.500000000000001e-05, 'epoch': 3.37}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 69%|██████▉   | 110/160 [10:08<04:32,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 69%|██████▉   | 111/160 [10:14<04:29,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 70%|███████   | 112/160 [10:20<04:29,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 71%|███████   | 113/160 [10:25<04:22,  5.58s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 71%|███████▏  | 114/160 [10:31<04:18,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 72%|███████▏  | 115/160 [10:37<04:17,  5.73s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 72%|███████▎  | 116/160 [10:42<04:05,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 73%|███████▎  | 117/160 [10:48<04:03,  5.67s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 74%|███████▍  | 118/160 [10:54<03:53,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 74%|███████▍  | 119/160 [10:59<03:47,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 75%|███████▌  | 120/160 [11:04<03:36,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.9126, 'grad_norm': 2.6036057472229004, 'learning_rate': 1.6543469682057106e-05, 'epoch': 3.68}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 75%|███████▌  | 120/160 [11:04<03:36,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 76%|███████▌  | 121/160 [11:10<03:31,  5.41s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 76%|███████▋  | 122/160 [11:15<03:22,  5.34s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 77%|███████▋  | 123/160 [11:20<03:16,  5.32s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 78%|███████▊  | 124/160 [11:25<03:12,  5.36s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 78%|███████▊  | 125/160 [11:31<03:07,  5.36s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 79%|███████▉  | 126/160 [11:37<03:06,  5.49s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 79%|███████▉  | 127/160 [11:42<03:00,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 80%|████████  | 128/160 [11:47<02:52,  5.40s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 81%|████████  | 129/160 [11:53<02:52,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 81%|████████▏ | 130/160 [11:59<02:47,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.8415, 'grad_norm': 2.5702261924743652, 'learning_rate': 9.549150281252633e-06, 'epoch': 3.98}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 81%|████████▏ | 130/160 [11:59<02:47,  5.59s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 82%|████████▏ | 131/160 [12:04<02:41,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 82%|████████▎ | 132/160 [12:10<02:35,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 83%|████████▎ | 133/160 [12:16<02:36,  5.78s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 84%|████████▍ | 134/160 [12:22<02:28,  5.72s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 84%|████████▍ | 135/160 [12:27<02:21,  5.66s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 85%|████████▌ | 136/160 [12:33<02:13,  5.56s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 86%|████████▌ | 137/160 [12:38<02:08,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 86%|████████▋ | 138/160 [12:44<02:04,  5.68s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 87%|████████▋ | 139/160 [12:50<01:59,  5.68s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 88%|████████▊ | 140/160 [12:55<01:51,  5.60s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.8629, 'grad_norm': 2.7346019744873047, 'learning_rate': 4.322727117869951e-06, 'epoch': 4.29}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 88%|████████▊ | 140/160 [12:55<01:51,  5.60s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 88%|████████▊ | 141/160 [13:01<01:46,  5.61s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 89%|████████▉ | 142/160 [13:06<01:41,  5.62s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 89%|████████▉ | 143/160 [13:12<01:32,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 90%|█████████ | 144/160 [13:17<01:29,  5.57s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 91%|█████████ | 145/160 [13:23<01:22,  5.48s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 91%|█████████▏| 146/160 [13:28<01:16,  5.50s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 92%|█████████▏| 147/160 [13:34<01:10,  5.46s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 92%|█████████▎| 148/160 [13:39<01:05,  5.45s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 93%|█████████▎| 149/160 [13:44<00:59,  5.39s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 94%|█████████▍| 150/160 [13:49<00:53,  5.32s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.7392, 'grad_norm': 2.5332913398742676, 'learning_rate': 1.0926199633097157e-06, 'epoch': 4.6}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 94%|█████████▍| 150/160 [13:49<00:53,  5.32s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 94%|█████████▍| 151/160 [13:55<00:48,  5.37s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 95%|█████████▌| 152/160 [14:01<00:43,  5.47s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 96%|█████████▌| 153/160 [14:06<00:38,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 96%|█████████▋| 154/160 [14:12<00:34,  5.69s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 97%|█████████▋| 155/160 [14:18<00:28,  5.65s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 98%|█████████▊| 156/160 [14:24<00:22,  5.74s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 98%|█████████▊| 157/160 [14:29<00:16,  5.51s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 99%|█████████▉| 158/160 [14:34<00:10,  5.44s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 99%|█████████▉| 159/160 [14:39<00:05,  5.43s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 100%|██████████| 160/160 [14:45<00:00,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'loss': 0.7989, 'grad_norm': 2.473985433578491, 'learning_rate': 0.0, 'epoch': 4.9}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | 100%|██████████| 160/160 [14:45<00:00,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3503] 2024-08-29 19:32:52,981 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-160\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3503] 2024-08-29 19:32:52,981 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-160\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:32:53,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:32:53,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:32:53,227 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:32:53,227 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2684] 2024-08-29 19:32:53,324 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2684] 2024-08-29 19:32:53,324 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2693] 2024-08-29 19:32:53,325 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2693] 2024-08-29 19:32:53,325 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2394] 2024-08-29 19:32:53,718 >> \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:2394] 2024-08-29 19:32:53,718 >> \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'train_runtime': 886.5015, 'train_samples_per_second': 1.472, 'train_steps_per_second': 0.18, 'train_loss': 1.2417689472436906, 'epoch': 4.9}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 100%|██████████| 160/160 [14:46<00:00,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 100%|██████████| 160/160 [14:46<00:00,  5.54s/it]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3503] 2024-08-29 19:32:53,721 >> Saving model checkpoint to /tmp/finetuned_model\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3503] 2024-08-29 19:32:53,721 >> Saving model checkpoint to /tmp/finetuned_model\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:32:53,995 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:733] 2024-08-29 19:32:53,995 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:32:53,996 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|configuration_utils.py:800] 2024-08-29 19:32:53,996 >> Model config LlamaConfig {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"architectures\": [\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"LlamaForCausalLM\"\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   ],\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"attention_dropout\": 0.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"bos_token_id\": 128000,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"eos_token_id\": 128001,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_act\": \"silu\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"hidden_size\": 4096,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"initializer_range\": 0.02,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"intermediate_size\": 14336,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"max_position_embeddings\": 8192,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"mlp_bias\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"model_type\": \"llama\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_attention_heads\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_hidden_layers\": 32,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"num_key_value_heads\": 8,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"pretraining_tp\": 1,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"quantization_config\": {\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"bits\": 4,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"group_size\": 128,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"modules_to_not_convert\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"quant_method\": \"awq\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"version\": \"gemm\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |     \"zero_point\": true\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   },\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rms_norm_eps\": 1e-05,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_scaling\": null,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"rope_theta\": 500000.0,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"tie_word_embeddings\": false,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"torch_dtype\": \"float16\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"transformers_version\": \"4.44.2\",\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"use_cache\": true,\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   \"vocab_size\": 128256\n",
      "kqh8jyxm6x-algo-1-ci1oq  | }\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2684] 2024-08-29 19:32:54,095 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2684] 2024-08-29 19:32:54,095 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2693] 2024-08-29 19:32:54,095 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|tokenization_utils_base.py:2693] 2024-08-29 19:32:54,095 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ***** train metrics *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   epoch                    =     4.9042\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   total_flos               =   552520GF\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   train_loss               =     1.2418\n",
      "kqh8jyxm6x-algo-1-ci1oq  | train_runtime            = 0:14:46.50\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   train_samples_per_second =      1.472\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   train_steps_per_second   =       0.18\n",
      "kqh8jyxm6x-algo-1-ci1oq  | Figure saved at: /tmp/finetuned_model/training_loss.png\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:32:54 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 08/29/2024 19:32:54 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3819] 2024-08-29 19:32:54,374 >> \n",
      "kqh8jyxm6x-algo-1-ci1oq  | ***** Running Evaluation *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3819] 2024-08-29 19:32:54,374 >> \n",
      "kqh8jyxm6x-algo-1-ci1oq  | ***** Running Evaluation *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3821] 2024-08-29 19:32:54,375 >>   Num examples = 30\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3824] 2024-08-29 19:32:54,375 >>   Batch size = 1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3821] 2024-08-29 19:32:54,375 >>   Num examples = 30\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|trainer.py:3824] 2024-08-29 19:32:54,375 >>   Batch size = 1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 0%|          | 0/30 [00:00<?, ?it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 10%|█         | 3/30 [00:00<00:01, 13.81it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 17%|█▋        | 5/30 [00:00<00:01, 13.14it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 23%|██▎       | 7/30 [00:00<00:01, 12.09it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 30%|███       | 9/30 [00:00<00:01, 12.27it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 37%|███▋      | 11/30 [00:00<00:01, 12.23it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 43%|████▎     | 13/30 [00:01<00:01, 11.02it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 50%|█████     | 15/30 [00:01<00:01, 10.49it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 57%|█████▋    | 17/30 [00:01<00:01, 10.57it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 63%|██████▎   | 19/30 [00:01<00:01, 10.75it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 70%|███████   | 21/30 [00:01<00:00, 10.73it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 77%|███████▋  | 23/30 [00:02<00:00, 10.59it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 83%|████████▎ | 25/30 [00:02<00:00, 10.95it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 90%|█████████ | 27/30 [00:02<00:00, 11.32it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 97%|█████████▋| 29/30 [00:02<00:00, 11.59it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 100%|██████████| 30/30 [00:02<00:00, 11.25it/s]\n",
      "kqh8jyxm6x-algo-1-ci1oq  | ***** eval metrics *****\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   epoch                   =     4.9042\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   eval_loss               =     1.5455\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   eval_runtime            = 0:00:02.78\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   eval_samples_per_second =     10.787\n",
      "kqh8jyxm6x-algo-1-ci1oq  |   eval_steps_per_second   =     10.787\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modelcard.py:449] 2024-08-29 19:32:57,158 >> Dropping the following result as it does not have all the necessary fields:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | [INFO|modelcard.py:449] 2024-08-29 19:32:57,158 >> Dropping the following result as it does not have all the necessary fields:\n",
      "kqh8jyxm6x-algo-1-ci1oq  | {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "kqh8jyxm6x-algo-1-ci1oq  | *****************finished training, start cp finetuned model*****************************\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/adapter_config.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/adapter_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/README.md s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/README.md\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/trainer_state.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/trainer_state.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/eval_results.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/eval_results.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/tokenizer_config.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/trainer_state.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/trainer_state.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/training_args.bin s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/training_args.bin\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/special_tokens_map.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/training_args.bin s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/training_args.bin\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/runs/Aug29_19-15-40_algo-1-ci1oq/events.out.tfevents.1724959087.algo-1-ci1oq.333.0 s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/runs/Aug29_19-15-40_algo-1-ci1oq/events.out.tfevents.1724959087.algo-1-ci1oq.333.0\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/rng_state.pth s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/rng_state.pth\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/runs/Aug29_19-15-40_algo-1-ci1oq/events.out.tfevents.1724959977.algo-1-ci1oq.333.1 s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/runs/Aug29_19-15-40_algo-1-ci1oq/events.out.tfevents.1724959977.algo-1-ci1oq.333.1\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/all_results.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/all_results.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/README.md s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/README.md\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/scheduler.pt s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/scheduler.pt\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/trainer_log.jsonl s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/trainer_log.jsonl\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/train_results.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/train_results.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/special_tokens_map.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/training_loss.png s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/training_loss.png\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/tokenizer_config.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/adapter_config.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/adapter_config.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/tokenizer.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/tokenizer.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/tokenizer.json s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/tokenizer.json\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/adapter_model.safetensors s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/adapter_model.safetensors\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/adapter_model.safetensors s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/adapter_model.safetensors\n",
      "kqh8jyxm6x-algo-1-ci1oq  | cp /tmp/finetuned_model/checkpoint-160/optimizer.pt s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/optimizer.pt\n",
      "kqh8jyxm6x-algo-1-ci1oq  | \n",
      "kqh8jyxm6x-algo-1-ci1oq  | -----finished cp-------\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:33:01,439 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:33:01,439 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "kqh8jyxm6x-algo-1-ci1oq  | 2024-08-29 19:33:01,439 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmpq2u9bdu0/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmpq2u9bdu0/algo-1-ci1oq/output/success -> /tmp/tmpq2u9bdu0/artifacts/output\n",
      "INFO:sagemaker.local.image:===== Job Complete =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kqh8jyxm6x-algo-1-ci1oq exited with code 0\n",
      "Aborting on container exit...\n",
      " Container kqh8jyxm6x-algo-1-ci1oq  Stopping\n",
      " Container kqh8jyxm6x-algo-1-ci1oq  Stopped\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-342367142984/llama3-8b-qlora/'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment['OUTPUT_MODEL_S3_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 至此部，本章节结束\n",
    "- 模型已经在本地训练完成，并上传至s3 位置在 : s3://{default_bucket}/llama3-8b-qlora/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
