{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLama Factory finetune on SageMaker \n",
    "# 1. Single GPU QLORA- 本地notebook实例训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pip/__main__.py\", line 8, in <module>\n",
      "    if sys.path[0] in (\"\", os.getcwd()):\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker boto3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pip/__main__.py\", line 8, in <module>\n",
      "    if sys.path[0] in (\"\", os.getcwd()):\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集1. 从huggingface上下载ruozhiba数据集\n",
    "- 改数据集有近5k条数据，本次实验我们可以只用前1k条做训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4898\n",
      "\n",
      "Training sample:\n",
      "\n",
      "{'input': '', 'instruction': '怎么才能把牙齿弄干净 到底要吃多少管牙膏才行', 'output': '把牙齿弄干净的方法很多，关键在于正确的口腔护理，而不是吃多少管牙膏。正确的刷牙方式包括用合适的牙刷和牙膏，采用正确的刷牙手法，每次刷牙至少两分钟，每天刷两次。此外，使用牙线、漱口水等工具帮助清洁牙齿缝隙也很重要。定期去牙医那里进行专业清洁和检查也是保持牙齿健康的重要措施。至于‘吃’牙膏，这是一个误解，牙膏的作用是在刷牙过程中帮助去除牙菌斑和牙垢，吃下牙膏不但没用，还有可能对身体有害，因为很多牙膏含有氟化物等化学成分，不适合内服。因此，保养牙齿重点在于正确的刷牙习惯和护理，而不是牙膏的量。'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "dataset_name = \"hfl/ruozhiba_gpt4\"\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train\",revision='41d2c61beb86c8d4c61916cc656c39d018c40ce5')\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集2. 身份数据集\n",
    "```json\n",
    "[{'instruction': 'hi',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'hello',\n",
    "  'input': '',\n",
    "  'output': 'Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'},\n",
    " {'instruction': 'Who are you?',\n",
    "  'input': '',\n",
    "  'output': 'I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?'}]\n",
    "```\n",
    "把其中的name和author替换成您自己想替换的值，这样微调完成之后，问模型“你是谁，谁创造的你？”这类的身份问题，模型就会按这个新的值来回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_identity(origin_obj,name,author):\n",
    "    ret = []\n",
    "    for ele in origin_obj:\n",
    "        ele['output'] = ele['output'].replace(\"{{name}}\",name).replace(\"{{author}}\",author)\n",
    "        ret.append(ele)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- 替换成您自己的设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME = 'AKABOT'\n",
    "AUTHOR = 'AWS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "pwd: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "pwd: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
      "/home/ec2-user/SageMaker/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "%cd ~/SageMaker/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'hi',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am AKABOT, an AI assistant developed by AWS. How can I assist you today?'},\n",
       " {'instruction': 'hello',\n",
       "  'input': '',\n",
       "  'output': 'Hello! I am AKABOT, an AI assistant developed by AWS. How can I assist you today?'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file_name = './LLaMA-Factory/data/identity.json'\n",
    "with open(file_name) as f:\n",
    "    identity = json.load(f)\n",
    "identity_2 = format_identity(identity,name=NAME,author=AUTHOR)\n",
    "identity_2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./train',exist_ok=True)\n",
    "with open('./train/identity_2.json','w') as f:\n",
    "    json.dump(identity_2,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 把数据copy至S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_data_uri = f\"s3://{default_bucket}/dataset-for-training\"\n",
    "training_input_path = f'{s3_data_uri}/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-342367142984/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "print(training_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c5f3abad834447badd3607fb67cd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset to: s3://sagemaker-us-east-1-342367142984/dataset-for-training/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "train_dataset.to_json('./train/ruozhiba.json')\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/ruozhiba.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train/identity_2.json\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = './LLaMA-Factory/data/dataset_info.json'\n",
    "with open(file_name) as f:\n",
    "    datainfo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['identity']={'file_name': 'identity_2.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datainfo['ruozhiba']={\n",
    "    'file_name':'ruozhiba.json',\n",
    "    \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\",\n",
    "  }      \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./LLaMA-Factory/data/dataset_info.json','w') as f:\n",
    "    json.dump(fp=f,obj=datainfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备LLaMA-Factory 的 训练配置yaml文件\n",
    "###  从LLaMA-Factory/examples/train_qlora/目录中复制出llama3_lora_sft_awq.yaml，并修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'dataset': 'identity,alpaca_en_demo',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 1024,\n",
       " 'max_samples': 1000,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': 'saves/llama3-8b/lora/sft',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 3.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load template\n",
    "import yaml\n",
    "file_name = './LLaMA-Factory/examples/train_qlora/llama3_lora_sft_awq.yaml'\n",
    "with open(file_name) as f:\n",
    "    doc = yaml.safe_load(f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#设置模型的保存目录在本notebook实例本地\n",
    "save_dir = '/home/ec2-user/SageMaker/Easy_Fintune_LLM_using_SageMaker_with_LLama_Factory/finetuned_model'\n",
    "# doc['output_dir'] = save_dir\n",
    "\n",
    "# 如果是用SageMaker则使用以下模型文件路径\n",
    "doc['output_dir'] ='/tmp/finetuned_model'\n",
    "doc['per_device_train_batch_size'] =1\n",
    "doc['gradient_accumulation_steps'] =8\n",
    "# doc['lora_target'] = 'all'\n",
    "doc['cutoff_len'] = 2048\n",
    "doc['num_train_epochs'] = 5.0\n",
    "doc['warmup_steps'] = 10\n",
    "\n",
    "#实验时间，只选取前200条数据做训练\n",
    "doc['max_samples'] = 200 \n",
    "#数据集\n",
    "doc['dataset'] = 'identity,ruozhiba'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存为训练配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name_or_path': 'TechxGenus/Meta-Llama-3-8B-Instruct-AWQ',\n",
       " 'stage': 'sft',\n",
       " 'do_train': True,\n",
       " 'finetuning_type': 'lora',\n",
       " 'lora_target': 'all',\n",
       " 'dataset': 'identity,ruozhiba',\n",
       " 'template': 'llama3',\n",
       " 'cutoff_len': 2048,\n",
       " 'max_samples': 200,\n",
       " 'overwrite_cache': True,\n",
       " 'preprocessing_num_workers': 16,\n",
       " 'output_dir': '/tmp/finetuned_model',\n",
       " 'logging_steps': 10,\n",
       " 'save_steps': 500,\n",
       " 'plot_loss': True,\n",
       " 'overwrite_output_dir': True,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_train_epochs': 5.0,\n",
       " 'lr_scheduler_type': 'cosine',\n",
       " 'warmup_ratio': 0.1,\n",
       " 'fp16': True,\n",
       " 'ddp_timeout': 180000000,\n",
       " 'val_size': 0.1,\n",
       " 'per_device_eval_batch_size': 1,\n",
       " 'eval_strategy': 'steps',\n",
       " 'eval_steps': 500,\n",
       " 'warmup_steps': 10}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_config = 'sg_config_qlora.yaml'\n",
    "with open(f'./LLaMA-Factory/{sg_config}', 'w') as f:\n",
    "    yaml.safe_dump(doc, f)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地GPU测试提交 Training job\n",
    "\n",
    "### 由于我们的实验环境限制，无法提交Training Job，所以在本次实验是在notebook实例中进行训练\n",
    "### 如果您在自己的AWS环境中，且有SageMaker Training Job 所需GPU实例的quota，则可以用如下代码提交，instance_type改成'ml.g5.2xlarge' \n",
    "\n",
    "```python\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "instance_count = 1\n",
    "instance_type = 'local_gpu' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-qlora-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    \"sg_config\":sg_config,\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{default_bucket}/llama3-8b-qlora/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry_single_lora.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)\n",
    "\n",
    "estimator.fit()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240710155757\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "#使用本地机器，也可以指定为 ml.g5.2xlarge等其他实例\n",
    "instance_type = 'local_gpu' \n",
    "max_time = 3600*24\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "# wandb.sagemaker_auth(path=\"./\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'llama3-8b-qlora-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    \"s3_data_paths\":f\"{training_input_path}\",\n",
    "    \"sg_config\":sg_config,\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{default_bucket}/llama3-8b-qlora/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry_single_lora.py',\n",
    "                            source_dir='./LLaMA-Factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.2.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            enable_remote_debug=True,\n",
    "                            # keep_alive_period_in_seconds=600,\n",
    "                            max_run=max_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.image:image pulled: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.2.0-gpu-py310\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmp1_2cnjfj/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container 5k8gfq1mzx-algo-1-cuufd  Creating\n",
      " Container 5k8gfq1mzx-algo-1-cuufd  Created\n",
      "Attaching to 5k8gfq1mzx-algo-1-cuufd\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:12,396 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:12,430 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:12,439 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:12,442 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:12,445 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:13,600 botocore.credentials INFO     Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:02:14,299 sagemaker-training-toolkit INFO     Installing module with the following command:\n",
      "5k8gfq1mzx-algo-1-cuufd  | /opt/conda/bin/python3.10 -m pip install . -r requirements.txt\n",
      "5k8gfq1mzx-algo-1-cuufd  | Processing /opt/ml/code\n",
      "5k8gfq1mzx-algo-1-cuufd  | Installing build dependencies: started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Installing build dependencies: finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Getting requirements to build wheel: started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Getting requirements to build wheel: finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (pyproject.toml): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting transformers>=4.41.2 (from -r requirements.txt (line 1))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.2 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting datasets>=2.16.0 (from -r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting accelerate>=0.30.1 (from -r requirements.txt (line 3))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting peft>=0.11.1 (from -r requirements.txt (line 4))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting trl>=0.8.6 (from -r requirements.txt (line 5))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting gradio>=4.0.0 (from -r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting sentencepiece (from -r requirements.txt (line 9))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting tiktoken (from -r requirements.txt (line 10))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting uvicorn (from -r requirements.txt (line 12))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting fastapi (from -r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting sse-starlette (from -r requirements.txt (line 15))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting fire (from -r requirements.txt (line 17))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 16.3 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting deepspeed (from -r requirements.txt (line 20))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading deepspeed-0.14.4.tar.gz (1.3 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 72.8 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting autoawq (from -r requirements.txt (line 21))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting metrics (from -r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.3.3.tar.gz (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting bitsandbytes (from -r requirements.txt (line 23))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting rouge-chinese (from -r requirements.txt (line 24))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting jieba (from -r requirements.txt (line 25))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 65.8 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting huggingface-hub<1.0,>=0.23.2 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting regex!=2019.12.17 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.1 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting safetensors>=0.4.1 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting pyarrow-hotfix (from datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting xxhash (from datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.5.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting aiohttp (from datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting gradio-client==1.0.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 9.0 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting urllib3~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 12))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting email_validator>=2.0.0 (from fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting anyio (from sse-starlette->-r requirements.txt (line 15))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting termcolor (from fire->-r requirements.txt (line 17))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting hjson (from deepspeed->-r requirements.txt (line 20))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 20))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading nvidia_ml_py-12.555.43-py3-none-any.whl.metadata (8.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting py-cpuinfo (from deepspeed->-r requirements.txt (line 20))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting autoawq-kernels (from autoawq->-r requirements.txt (line 21))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting Pygments==2.2.0 (from metrics->-r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting pathspec==0.5.5 (from metrics->-r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading pathspec-0.5.5.tar.gz (21 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting pathlib2>=2.3.0 (from metrics->-r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting sniffio (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | INFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | INFO: pip is still looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting markdown-it-py<3.0.0,>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.5-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.4-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting commonmark<0.10.0,>=0.9.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.0.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-13.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.4.4-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.4.3-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-12.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-11.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-11.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-11.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.16.2-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.16.1-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.16.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.15.2-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.15.1-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.15.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.14.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.13.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading rich-10.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting metrics (from -r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.3.2.tar.gz (18 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.3.1.tar.gz (14 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.3.0.tar.gz (14 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.2.8.tar.gz (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting pathspec==0.5.3 (from metrics->-r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading pathspec-0.5.3.tar.gz (20 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Collecting metrics (from -r requirements.txt (line 22))\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.2.7.tar.gz (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading metrics-0.2.6.tar.gz (12 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing metadata (setup.py): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 103.0 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 47.6 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 314.1/314.1 kB 41.8 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 29.1 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 38.5 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 88.7 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.2/318.2 kB 46.9 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 96.1 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 75.0 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.4/62.4 kB 12.4 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 13.0 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 15.2 MB/s eta 0:00:00\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Attempting uninstall: accelerate\n",
      "5k8gfq1mzx-algo-1-cuufd  | Found existing installation: accelerate 0.22.0\n",
      "5k8gfq1mzx-algo-1-cuufd  | Uninstalling accelerate-0.22.0:\n",
      "5k8gfq1mzx-algo-1-cuufd  | Successfully uninstalled accelerate-0.22.0\n",
      "5k8gfq1mzx-algo-1-cuufd  | ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "5k8gfq1mzx-algo-1-cuufd  | spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "5k8gfq1mzx-algo-1-cuufd  | weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
      "5k8gfq1mzx-algo-1-cuufd  | Successfully installed accelerate-0.32.1 aiofiles-23.2.1 aiohttp-3.9.5 aiosignal-1.3.1 altair-5.3.0 anyio-4.4.0 async-timeout-4.0.3 autoawq-0.2.5 autoawq-kernels-0.0.6 bitsandbytes-0.43.1 datasets-2.20.0 deepspeed-0.14.4 dnspython-2.6.1 docstring-parser-0.16 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 frozenlist-1.4.1 gradio-4.37.2 gradio-client-1.0.2 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.23.4 importlib-resources-6.4.0 jieba-0.42.1 llamafactory-0.8.2.dev0 metrics-0.2.6 multidict-6.0.5 nvidia-ml-py-12.555.43 orjson-3.10.6 peft-0.11.1 py-cpuinfo-9.0.0 pyarrow-hotfix-0.6 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 rouge-chinese-1.0.3 ruff-0.5.1 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shtab-1.7.1 sniffio-1.3.1 sse-starlette-2.1.2 starlette-0.37.2 termcolor-2.4.0 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.42.3 trl-0.9.6 typer-0.12.3 tyro-0.8.5 ujson-5.10.0 urllib3-2.2.2 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1 yarl-1.9.4\n",
      "5k8gfq1mzx-algo-1-cuufd  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] To update, run: pip install --upgrade pip\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:05,993 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:05,993 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,052 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,060 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,101 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,110 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,149 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,157 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,161 sagemaker-training-toolkit INFO     Invoking user script\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Training Env:\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"additional_framework_parameters\": {},\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"channel_input_dirs\": {},\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"current_host\": \"algo-1-cuufd\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"current_instance_group\": \"homogeneousCluster\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"current_instance_group_hosts\": [],\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"current_instance_type\": \"local\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"distribution_hosts\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |         \"algo-1-cuufd\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |     ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"distribution_instance_groups\": [],\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"hosts\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |         \"algo-1-cuufd\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |     ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"hyperparameters\": {},\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"input_data_config\": {},\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"input_dir\": \"/opt/ml/input\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"instance_groups\": [],\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"instance_groups_dict\": {},\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"is_hetero\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"is_master\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"is_modelparallel_enabled\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"is_smddpmprun_installed\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"is_smddprun_installed\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"job_name\": \"llama3-8b-qlora-finetune-2024-07-10-15-58-04-387\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"log_level\": 20,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"master_hostname\": \"algo-1-cuufd\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"model_dir\": \"/opt/ml/model\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"module_dir\": \"s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora-finetune-2024-07-10-15-58-04-387/source/sourcedir.tar.gz\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"module_name\": \"entry_single_lora\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"network_interface_name\": \"eth0\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"num_cpus\": 96,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"num_gpus\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"num_neurons\": 0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"output_dir\": \"/opt/ml/output\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"resource_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |         \"current_host\": \"algo-1-cuufd\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |         \"hosts\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |             \"algo-1-cuufd\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |         ]\n",
      "5k8gfq1mzx-algo-1-cuufd  |     },\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"user_entry_point\": \"entry_single_lora.py\"\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Environment variables:\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_HOSTS=[\"algo-1-cuufd\"]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_NETWORK_INTERFACE_NAME=eth0\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_HPS={}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_USER_ENTRY_POINT=entry_single_lora.py\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_FRAMEWORK_PARAMS={}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-cuufd\",\"hosts\":[\"algo-1-cuufd\"]}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_INPUT_DATA_CONFIG={}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_CHANNELS=[]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_CURRENT_HOST=algo-1-cuufd\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_CURRENT_INSTANCE_TYPE=local\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_INSTANCE_GROUPS=[]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_INSTANCE_GROUPS_DICT={}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_IS_HETERO=false\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_MODULE_NAME=entry_single_lora\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_LOG_LEVEL=20\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_INPUT_DIR=/opt/ml/input\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_OUTPUT_DIR=/opt/ml/output\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_NUM_CPUS=96\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_NUM_GPUS=4\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_NUM_NEURONS=0\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_MODEL_DIR=/opt/ml/model\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_MODULE_DIR=s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora-finetune-2024-07-10-15-58-04-387/source/sourcedir.tar.gz\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1-cuufd\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-cuufd\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-cuufd\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"llama3-8b-qlora-finetune-2024-07-10-15-58-04-387\",\"log_level\":20,\"master_hostname\":\"algo-1-cuufd\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora-finetune-2024-07-10-15-58-04-387/source/sourcedir.tar.gz\",\"module_name\":\"entry_single_lora\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-cuufd\",\"hosts\":[\"algo-1-cuufd\"]},\"user_entry_point\":\"entry_single_lora.py\"}\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_USER_ARGS=[]\n",
      "5k8gfq1mzx-algo-1-cuufd  | SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "5k8gfq1mzx-algo-1-cuufd  | PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Invoking script with the following command:\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | /opt/conda/bin/python3.10 -m entry_single_lora\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,163 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:03:06,163 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "5k8gfq1mzx-algo-1-cuufd  | Obtaining file:///opt/ml/code\n",
      "5k8gfq1mzx-algo-1-cuufd  | Installing build dependencies: started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Installing build dependencies: finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Checking if build backend supports build_editable: started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Checking if build backend supports build_editable: finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Getting requirements to build editable: started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Getting requirements to build editable: finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing editable metadata (pyproject.toml): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Building wheels for collected packages: llamafactory\n",
      "5k8gfq1mzx-algo-1-cuufd  | Building editable for llamafactory (pyproject.toml): started\n",
      "5k8gfq1mzx-algo-1-cuufd  | Building editable for llamafactory (pyproject.toml): finished with status 'done'\n",
      "5k8gfq1mzx-algo-1-cuufd  | Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=18802 sha256=31261490061b0312ffd8c9b1c8fd45ebd9f31fcd5c85b8c4c16c18491bcb622f\n",
      "5k8gfq1mzx-algo-1-cuufd  | Stored in directory: /tmp/pip-ephem-wheel-cache-0ku9p734/wheels/ee/79/1e/3fb168dd34359b627e23b53045c3eb498188294150b39e2fb0\n",
      "5k8gfq1mzx-algo-1-cuufd  | Successfully built llamafactory\n",
      "5k8gfq1mzx-algo-1-cuufd  | Installing collected packages: llamafactory\n",
      "5k8gfq1mzx-algo-1-cuufd  | Attempting uninstall: llamafactory\n",
      "5k8gfq1mzx-algo-1-cuufd  | Found existing installation: llamafactory 0.8.2.dev0\n",
      "5k8gfq1mzx-algo-1-cuufd  | Uninstalling llamafactory-0.8.2.dev0:\n",
      "5k8gfq1mzx-algo-1-cuufd  | Successfully uninstalled llamafactory-0.8.2.dev0\n",
      "5k8gfq1mzx-algo-1-cuufd  | Successfully installed llamafactory-0.8.2.dev0\n",
      "5k8gfq1mzx-algo-1-cuufd  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] To update, run: pip install --upgrade pip\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.42.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.20.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.32.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: peft>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.11.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: trl>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.9.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: gradio>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.37.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.8.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.7.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.30.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.7.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.111.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: sse-starlette in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.1.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fire in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.6.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (23.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (0.14.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: autoawq in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (0.2.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: metrics in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.2.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.43.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: rouge-chinese in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (1.0.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (0.42.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.14.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.23.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2024.5.15)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.4.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.19.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (16.1.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (2.2.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.5.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl>=0.8.6->-r requirements.txt (line 5)) (0.8.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (23.2.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (5.3.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.3.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: gradio-client==1.0.2 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.10.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.25.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.0.9)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.5.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.10.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.11.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.2.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.0.2->gradio>=4.0.0->-r requirements.txt (line 6)) (11.0.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (0.14.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.7.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.18.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.37.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (0.0.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (5.10.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (2.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 15)) (4.4.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.52.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (2.4.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (3.1.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (1.11.1.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (12.555.43)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (9.0.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: zstandard in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.22.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: autoawq-kernels in /opt/conda/lib/python3.10/site-packages (from autoawq->-r requirements.txt (line 21)) (0.0.6)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: Pygments>=0.8 in /opt/conda/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.18.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.22.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (2.6.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: idna>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.7)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.5)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.3)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: docstring-parser>=0.16 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (0.16)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5)) (1.7.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.6.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (1.0.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.19.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14)) (0.22.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.35.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.1)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "5k8gfq1mzx-algo-1-cuufd  | Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "5k8gfq1mzx-algo-1-cuufd  | WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [notice] To update, run: pip install --upgrade pip\n",
      "5k8gfq1mzx-algo-1-cuufd  | CITATION.cff\n",
      "5k8gfq1mzx-algo-1-cuufd  | Dockerfile\n",
      "5k8gfq1mzx-algo-1-cuufd  | LICENSE\n",
      "5k8gfq1mzx-algo-1-cuufd  | MANIFEST.in\n",
      "5k8gfq1mzx-algo-1-cuufd  | Makefile\n",
      "5k8gfq1mzx-algo-1-cuufd  | README.md\n",
      "5k8gfq1mzx-algo-1-cuufd  | README_zh.md\n",
      "5k8gfq1mzx-algo-1-cuufd  | assets\n",
      "5k8gfq1mzx-algo-1-cuufd  | build\n",
      "5k8gfq1mzx-algo-1-cuufd  | data\n",
      "5k8gfq1mzx-algo-1-cuufd  | docker-compose.yml\n",
      "5k8gfq1mzx-algo-1-cuufd  | entry-multi-nodes.py\n",
      "5k8gfq1mzx-algo-1-cuufd  | entry_single_lora.py\n",
      "5k8gfq1mzx-algo-1-cuufd  | evaluation\n",
      "5k8gfq1mzx-algo-1-cuufd  | examples\n",
      "5k8gfq1mzx-algo-1-cuufd  | pyproject.toml\n",
      "5k8gfq1mzx-algo-1-cuufd  | requirements.txt\n",
      "5k8gfq1mzx-algo-1-cuufd  | s5cmd\n",
      "5k8gfq1mzx-algo-1-cuufd  | scripts\n",
      "5k8gfq1mzx-algo-1-cuufd  | setup.py\n",
      "5k8gfq1mzx-algo-1-cuufd  | sg_config_qlora.yaml\n",
      "5k8gfq1mzx-algo-1-cuufd  | src\n",
      "5k8gfq1mzx-algo-1-cuufd  | tests\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp s3://sagemaker-us-east-1-342367142984/dataset-for-training/train/identity_2.json /opt/ml/code/data/identity_2.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp s3://sagemaker-us-east-1-342367142984/dataset-for-training/train/ruozhiba.json /opt/ml/code/data/ruozhiba.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [2024-07-10 16:03:27,318] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "5k8gfq1mzx-algo-1-cuufd  | df: /root/.triton/autotune\n",
      "5k8gfq1mzx-algo-1-cuufd  | : No such file or directory\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "5k8gfq1mzx-algo-1-cuufd  | \u001b[93m [WARNING] \u001b[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:29 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file added_tokens.json from cache at None\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file added_tokens.json from cache at None\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2161] 2024-07-10 16:03:30,085 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [WARNING|logging.py:313] 2024-07-10 16:03:30,380 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [WARNING|logging.py:313] 2024-07-10 16:03:30,380 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:30 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:30 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:30 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\n",
      "5k8gfq1mzx-algo-1-cuufd  | Generating train split: 0 examples [00:00, ? examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Generating train split: 91 examples [00:00, 9175.04 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16):   0%|          | 0/91 [00:00<?, ? examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16):  78%|███████▊  | 71/91 [00:00<00:00, 682.08 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 487.84 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:30 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\n",
      "5k8gfq1mzx-algo-1-cuufd  | Generating train split: 0 examples [00:00, ? examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Generating train split: 4898 examples [00:00, 263627.51 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16):  82%|████████▏ | 164/200 [00:00<00:00, 1634.41 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Converting format of dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 1096.60 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):   0%|          | 0/291 [00:00<?, ? examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):   7%|▋         | 19/291 [00:00<00:07, 35.81 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  13%|█▎        | 38/291 [00:00<00:04, 60.15 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  20%|█▉        | 57/291 [00:00<00:03, 77.59 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  26%|██▌       | 75/291 [00:01<00:02, 87.31 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  32%|███▏      | 93/291 [00:01<00:02, 94.54 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  38%|███▊      | 111/291 [00:01<00:01, 98.59 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  44%|████▍     | 129/291 [00:01<00:01, 101.63 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  51%|█████     | 147/291 [00:01<00:01, 105.07 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 165/291 [00:01<00:01, 106.75 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 183/291 [00:02<00:01, 107.64 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 201/291 [00:02<00:00, 108.72 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 219/291 [00:02<00:00, 109.02 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 237/291 [00:02<00:00, 109.54 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 255/291 [00:02<00:00, 110.12 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 273/291 [00:02<00:00, 110.24 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16): 100%|██████████| 291/291 [00:02<00:00, 108.89 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Running tokenizer on dataset (num_proc=16): 100%|██████████| 291/291 [00:03<00:00, 95.11 examples/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | input_ids:\n",
      "5k8gfq1mzx-algo-1-cuufd  | [128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 31672, 1905, 1831, 11, 459, 15592, 18328, 8040, 555, 24124, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "5k8gfq1mzx-algo-1-cuufd  | inputs:\n",
      "5k8gfq1mzx-algo-1-cuufd  | <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Hello! I am AKABOT, an AI assistant developed by AWS. How can I assist you today?<|eot_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | label_ids:\n",
      "5k8gfq1mzx-algo-1-cuufd  | [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 31672, 1905, 1831, 11, 459, 15592, 18328, 8040, 555, 24124, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "5k8gfq1mzx-algo-1-cuufd  | labels:\n",
      "5k8gfq1mzx-algo-1-cuufd  | Hello! I am AKABOT, an AI assistant developed by AWS. How can I assist you today?<|eot_id|>\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:03:35,060 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:03:35,060 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:03:35,061 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:03:35,061 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:03:35 - INFO - llamafactory.model.model_utils.quantization - Loading 4-bit AWQ-quantized model.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:3556] 2024-07-10 16:03:35,187 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/model.safetensors.index.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:3556] 2024-07-10 16:03:35,187 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/model.safetensors.index.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading shards:  50%|█████     | 1/2 [00:18<00:18, 18.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading shards: 100%|██████████| 2/2 [00:23<00:00, 10.25s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Downloading shards: 100%|██████████| 2/2 [00:23<00:00, 11.54s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:1531] 2024-07-10 16:03:58,268 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:1531] 2024-07-10 16:03:58,268 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:1000] 2024-07-10 16:03:58,269 >> Generate config GenerationConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:1000] 2024-07-10 16:03:58,269 >> Generate config GenerationConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  2.00s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:4364] 2024-07-10 16:04:01,104 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:4364] 2024-07-10 16:04:01,104 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:4372] 2024-07-10 16:04:01,105 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TechxGenus/Meta-Llama-3-8B-Instruct-AWQ.\n",
      "5k8gfq1mzx-algo-1-cuufd  | If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modeling_utils.py:4372] 2024-07-10 16:04:01,105 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TechxGenus/Meta-Llama-3-8B-Instruct-AWQ.\n",
      "5k8gfq1mzx-algo-1-cuufd  | If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:955] 2024-07-10 16:04:01,150 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/generation_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:955] 2024-07-10 16:04:01,150 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/generation_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:1000] 2024-07-10 16:04:01,150 >> Generate config GenerationConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"do_sample\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     128009\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ]\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:1000] 2024-07-10 16:04:01,150 >> Generate config GenerationConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"do_sample\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     128009\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ]\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,gate_proj,v_proj,down_proj,up_proj,k_proj\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:04:01 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 1071910912 || trainable%: 1.9565\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:642] 2024-07-10 16:04:01,777 >> Using auto half precision backend\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:642] 2024-07-10 16:04:01,777 >> Using auto half precision backend\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2128] 2024-07-10 16:04:02,109 >> ***** Running training *****\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2129] 2024-07-10 16:04:02,109 >>   Num examples = 261\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2130] 2024-07-10 16:04:02,109 >>   Num Epochs = 5\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2128] 2024-07-10 16:04:02,109 >> ***** Running training *****\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2129] 2024-07-10 16:04:02,109 >>   Num examples = 261\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2130] 2024-07-10 16:04:02,109 >>   Num Epochs = 5\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2131] 2024-07-10 16:04:02,109 >>   Instantaneous batch size per device = 1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2134] 2024-07-10 16:04:02,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2135] 2024-07-10 16:04:02,109 >>   Gradient Accumulation steps = 8\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2136] 2024-07-10 16:04:02,109 >>   Total optimization steps = 160\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2131] 2024-07-10 16:04:02,109 >>   Instantaneous batch size per device = 1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2134] 2024-07-10 16:04:02,109 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2135] 2024-07-10 16:04:02,109 >>   Gradient Accumulation steps = 8\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2136] 2024-07-10 16:04:02,109 >>   Total optimization steps = 160\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2137] 2024-07-10 16:04:02,114 >>   Number of trainable parameters = 20,971,520\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2137] 2024-07-10 16:04:02,114 >>   Number of trainable parameters = 20,971,520\n",
      "5k8gfq1mzx-algo-1-cuufd  | 0%|          | 0/160 [00:00<?, ?it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 1%|          | 1/160 [00:07<18:49,  7.10s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 1%|▏         | 2/160 [00:12<16:48,  6.39s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2%|▏         | 3/160 [00:18<16:07,  6.16s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2%|▎         | 4/160 [00:24<15:44,  6.05s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 3%|▎         | 5/160 [00:31<15:53,  6.15s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 4%|▍         | 6/160 [00:37<15:46,  6.14s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 4%|▍         | 7/160 [00:42<15:20,  6.02s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 5%|▌         | 8/160 [00:48<15:07,  5.97s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 6%|▌         | 9/160 [00:53<14:13,  5.65s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 6%|▋         | 10/160 [00:59<14:28,  5.79s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 2.2927, 'grad_norm': 1.6243823766708374, 'learning_rate': 9e-05, 'epoch': 0.31}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 6%|▋         | 10/160 [00:59<14:28,  5.79s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 7%|▋         | 11/160 [01:05<14:28,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 8%|▊         | 12/160 [01:11<14:23,  5.84s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 8%|▊         | 13/160 [01:17<14:08,  5.77s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 9%|▉         | 14/160 [01:22<13:58,  5.74s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 9%|▉         | 15/160 [01:28<13:47,  5.71s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 10%|█         | 16/160 [01:34<13:48,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 11%|█         | 17/160 [01:40<13:38,  5.73s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 11%|█▏        | 18/160 [01:46<13:50,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 12%|█▏        | 19/160 [01:52<13:53,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 12%|█▎        | 20/160 [01:58<13:48,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.8305, 'grad_norm': 1.8604336977005005, 'learning_rate': 9.911436253643445e-05, 'epoch': 0.61}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 12%|█▎        | 20/160 [01:58<13:48,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 13%|█▎        | 21/160 [02:03<13:24,  5.79s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 14%|█▍        | 22/160 [02:09<13:10,  5.73s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 14%|█▍        | 23/160 [02:14<13:02,  5.71s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 15%|█▌        | 24/160 [02:20<13:08,  5.80s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 16%|█▌        | 25/160 [02:26<12:58,  5.77s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 16%|█▋        | 26/160 [02:32<12:48,  5.73s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 17%|█▋        | 27/160 [02:38<12:45,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 18%|█▊        | 28/160 [02:44<12:45,  5.80s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 18%|█▊        | 29/160 [02:49<12:34,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 19%|█▉        | 30/160 [02:56<12:52,  5.94s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.6072, 'grad_norm': 1.77174973487854, 'learning_rate': 9.609315757942503e-05, 'epoch': 0.92}\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | 19%|█▉        | 30/160 [02:56<12:52,  5.94s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 19%|█▉        | 31/160 [03:02<12:51,  5.98s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 20%|██        | 32/160 [03:07<12:29,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 21%|██        | 33/160 [03:13<12:31,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 21%|██▏       | 34/160 [03:19<12:08,  5.78s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 22%|██▏       | 35/160 [03:24<11:51,  5.69s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 22%|██▎       | 36/160 [03:30<11:36,  5.61s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 23%|██▎       | 37/160 [03:36<11:55,  5.82s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 24%|██▍       | 38/160 [03:42<12:00,  5.90s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 24%|██▍       | 39/160 [03:48<11:49,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 25%|██▌       | 40/160 [03:54<11:39,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.4563, 'grad_norm': 1.9437392950057983, 'learning_rate': 9.105746045668521e-05, 'epoch': 1.23}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 25%|██▌       | 40/160 [03:54<11:39,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 26%|██▌       | 41/160 [04:00<11:43,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 26%|██▋       | 42/160 [04:05<11:33,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 27%|██▋       | 43/160 [04:11<11:26,  5.87s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 28%|██▊       | 44/160 [04:17<11:15,  5.82s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 28%|██▊       | 45/160 [04:23<11:15,  5.87s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 29%|██▉       | 46/160 [04:29<11:14,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 29%|██▉       | 47/160 [04:35<11:15,  5.98s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 30%|███       | 48/160 [04:41<11:07,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 31%|███       | 49/160 [04:47<10:57,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 31%|███▏      | 50/160 [04:53<10:54,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.384, 'grad_norm': 1.9728977680206299, 'learning_rate': 8.422735529643444e-05, 'epoch': 1.53}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 31%|███▏      | 50/160 [04:53<10:54,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 32%|███▏      | 51/160 [04:59<10:55,  6.02s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 32%|███▎      | 52/160 [05:04<10:29,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 33%|███▎      | 53/160 [05:10<10:25,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 34%|███▍      | 54/160 [05:16<10:20,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 34%|███▍      | 55/160 [05:22<10:13,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 35%|███▌      | 56/160 [05:28<10:12,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 36%|███▌      | 57/160 [05:34<10:11,  5.93s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 36%|███▋      | 58/160 [05:40<09:54,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 37%|███▋      | 59/160 [05:46<09:55,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 38%|███▊      | 60/160 [05:52<09:46,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.3817, 'grad_norm': 2.2157528400421143, 'learning_rate': 7.590135046865651e-05, 'epoch': 1.84}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 38%|███▊      | 60/160 [05:52<09:46,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 38%|███▊      | 61/160 [05:58<09:47,  5.94s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 39%|███▉      | 62/160 [06:03<09:31,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 39%|███▉      | 63/160 [06:09<09:30,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 40%|████      | 64/160 [06:15<09:12,  5.75s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 41%|████      | 65/160 [06:21<09:19,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 41%|████▏     | 66/160 [06:27<09:19,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 42%|████▏     | 67/160 [06:33<09:09,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 42%|████▎     | 68/160 [06:39<08:58,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 43%|████▎     | 69/160 [06:44<08:51,  5.84s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 44%|████▍     | 70/160 [06:50<08:46,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.254, 'grad_norm': 1.9175201654434204, 'learning_rate': 6.644333233692916e-05, 'epoch': 2.15}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 44%|████▍     | 70/160 [06:50<08:46,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 44%|████▍     | 71/160 [06:57<08:56,  6.03s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 45%|████▌     | 72/160 [07:02<08:39,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 46%|████▌     | 73/160 [07:08<08:25,  5.81s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 46%|████▋     | 74/160 [07:14<08:22,  5.84s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 47%|████▋     | 75/160 [07:20<08:20,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 48%|████▊     | 76/160 [07:26<08:15,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 48%|████▊     | 77/160 [07:32<08:13,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 49%|████▉     | 78/160 [07:38<08:05,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 49%|████▉     | 79/160 [07:43<07:55,  5.87s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 50%|█████     | 80/160 [07:49<07:48,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.1313, 'grad_norm': 2.385258674621582, 'learning_rate': 5.6266661678215216e-05, 'epoch': 2.45}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 50%|█████     | 80/160 [07:49<07:48,  5.85s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 51%|█████     | 81/160 [07:55<07:46,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 51%|█████▏    | 82/160 [08:01<07:37,  5.87s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 52%|█████▏    | 83/160 [08:07<07:34,  5.90s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 52%|█████▎    | 84/160 [08:13<07:31,  5.94s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 53%|█████▎    | 85/160 [08:19<07:24,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 54%|█████▍    | 86/160 [08:24<07:10,  5.82s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 54%|█████▍    | 87/160 [08:30<06:57,  5.73s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 55%|█████▌    | 88/160 [08:35<06:47,  5.66s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 56%|█████▌    | 89/160 [08:41<06:48,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 56%|█████▋    | 90/160 [08:47<06:46,  5.81s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.103, 'grad_norm': 2.5260140895843506, 'learning_rate': 4.5816107833384234e-05, 'epoch': 2.76}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 56%|█████▋    | 90/160 [08:47<06:46,  5.81s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 57%|█████▋    | 91/160 [08:54<06:48,  5.93s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 57%|█████▊    | 92/160 [09:00<06:46,  5.98s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 58%|█████▊    | 93/160 [09:05<06:34,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 59%|█████▉    | 94/160 [09:11<06:32,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 59%|█████▉    | 95/160 [09:17<06:27,  5.97s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 60%|██████    | 96/160 [09:23<06:16,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 61%|██████    | 97/160 [09:28<05:56,  5.66s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 61%|██████▏   | 98/160 [09:34<05:55,  5.74s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 62%|██████▏   | 99/160 [09:40<05:48,  5.71s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 62%|██████▎   | 100/160 [09:46<05:47,  5.79s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 1.0477, 'grad_norm': 2.131551504135132, 'learning_rate': 3.554841015277641e-05, 'epoch': 3.07}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 62%|██████▎   | 100/160 [09:46<05:47,  5.79s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 63%|██████▎   | 101/160 [09:52<05:55,  6.02s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 64%|██████▍   | 102/160 [09:59<05:50,  6.05s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 64%|██████▍   | 103/160 [10:04<05:37,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 65%|██████▌   | 104/160 [10:10<05:31,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 66%|██████▌   | 105/160 [10:16<05:22,  5.86s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 66%|██████▋   | 106/160 [10:22<05:22,  5.98s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 67%|██████▋   | 107/160 [10:27<05:02,  5.72s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 68%|██████▊   | 108/160 [10:33<04:53,  5.64s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 68%|██████▊   | 109/160 [10:39<04:52,  5.74s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 69%|██████▉   | 110/160 [10:44<04:48,  5.77s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.8934, 'grad_norm': 2.554927110671997, 'learning_rate': 2.591231629491423e-05, 'epoch': 3.37}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 69%|██████▉   | 110/160 [10:44<04:48,  5.77s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 69%|██████▉   | 111/160 [10:50<04:45,  5.82s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 70%|███████   | 112/160 [10:57<04:46,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 71%|███████   | 113/160 [11:02<04:37,  5.90s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 71%|███████▏  | 114/160 [11:09<04:35,  5.98s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 72%|███████▏  | 115/160 [11:15<04:35,  6.12s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 72%|███████▎  | 116/160 [11:20<04:20,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 73%|███████▎  | 117/160 [11:27<04:19,  6.02s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 74%|███████▍  | 118/160 [11:32<04:08,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 74%|███████▍  | 119/160 [11:38<04:01,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 75%|███████▌  | 120/160 [11:44<03:49,  5.75s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.8938, 'grad_norm': 2.6475777626037598, 'learning_rate': 1.7328969800494726e-05, 'epoch': 3.68}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 75%|███████▌  | 120/160 [11:44<03:49,  5.75s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 76%|███████▌  | 121/160 [11:49<03:44,  5.74s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 76%|███████▋  | 122/160 [11:55<03:34,  5.65s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 77%|███████▋  | 123/160 [12:00<03:28,  5.63s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 78%|███████▊  | 124/160 [12:06<03:23,  5.65s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 78%|███████▊  | 125/160 [12:12<03:17,  5.65s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 79%|███████▉  | 126/160 [12:18<03:18,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 79%|███████▉  | 127/160 [12:24<03:12,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 80%|████████  | 128/160 [12:29<03:04,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 81%|████████  | 129/160 [12:36<03:04,  5.94s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 81%|████████▏ | 130/160 [12:42<02:58,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.8343, 'grad_norm': 2.7427237033843994, 'learning_rate': 1.0173504098790187e-05, 'epoch': 3.98}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 81%|████████▏ | 130/160 [12:42<02:58,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 82%|████████▏ | 131/160 [12:48<02:51,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 82%|████████▎ | 132/160 [12:53<02:44,  5.89s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 83%|████████▎ | 133/160 [13:00<02:46,  6.16s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 84%|████████▍ | 134/160 [13:06<02:38,  6.09s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 84%|████████▍ | 135/160 [13:12<02:30,  6.04s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 85%|████████▌ | 136/160 [13:18<02:22,  5.92s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 86%|████████▌ | 137/160 [13:24<02:15,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 86%|████████▋ | 138/160 [13:30<02:12,  6.03s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 87%|████████▋ | 139/160 [13:36<02:06,  6.03s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 88%|████████▊ | 140/160 [13:42<01:58,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.8469, 'grad_norm': 2.8970892429351807, 'learning_rate': 4.758647376699032e-06, 'epoch': 4.29}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 88%|████████▊ | 140/160 [13:42<01:58,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 88%|████████▊ | 141/160 [13:48<01:53,  5.95s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 89%|████████▉ | 142/160 [13:54<01:47,  5.96s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 89%|████████▉ | 143/160 [13:59<01:38,  5.77s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 90%|█████████ | 144/160 [14:05<01:34,  5.91s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 91%|█████████ | 145/160 [14:11<01:27,  5.82s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 91%|█████████▏| 146/160 [14:17<01:21,  5.83s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 92%|█████████▏| 147/160 [14:22<01:15,  5.78s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 92%|█████████▎| 148/160 [14:28<01:09,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 93%|█████████▎| 149/160 [14:33<01:02,  5.69s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 94%|█████████▍| 150/160 [14:39<00:56,  5.64s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.7164, 'grad_norm': 2.594536066055298, 'learning_rate': 1.3210548563419856e-06, 'epoch': 4.6}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 94%|█████████▍| 150/160 [14:39<00:56,  5.64s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 94%|█████████▍| 151/160 [14:45<00:51,  5.68s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 95%|█████████▌| 152/160 [14:51<00:46,  5.81s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 96%|█████████▌| 153/160 [14:57<00:41,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 96%|█████████▋| 154/160 [15:04<00:36,  6.08s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 97%|█████████▋| 155/160 [15:09<00:30,  6.03s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 98%|█████████▊| 156/160 [15:16<00:24,  6.13s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 98%|█████████▊| 157/160 [15:21<00:17,  5.84s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 99%|█████████▉| 158/160 [15:27<00:11,  5.76s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 99%|█████████▉| 159/160 [15:32<00:05,  5.74s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 100%|██████████| 160/160 [15:38<00:00,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'loss': 0.7795, 'grad_norm': 2.499972343444824, 'learning_rate': 1.096582625772502e-08, 'epoch': 4.9}\n",
      "5k8gfq1mzx-algo-1-cuufd  | 100%|██████████| 160/160 [15:38<00:00,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3478] 2024-07-10 16:19:41,043 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-160\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3478] 2024-07-10 16:19:41,043 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-160\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:19:41,138 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:19:41,138 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:19:41,139 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:19:41,139 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2574] 2024-07-10 16:19:41,271 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2574] 2024-07-10 16:19:41,271 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2583] 2024-07-10 16:19:41,271 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2583] 2024-07-10 16:19:41,271 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2383] 2024-07-10 16:19:41,664 >> \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:2383] 2024-07-10 16:19:41,664 >> \n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | {'train_runtime': 939.5508, 'train_samples_per_second': 1.389, 'train_steps_per_second': 0.17, 'train_loss': 1.2157954037189485, 'epoch': 4.9}\n",
      "5k8gfq1mzx-algo-1-cuufd  | \n",
      "5k8gfq1mzx-algo-1-cuufd  | 100%|██████████| 160/160 [15:39<00:00,  5.88s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 100%|██████████| 160/160 [15:39<00:00,  5.87s/it]\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3478] 2024-07-10 16:19:41,667 >> Saving model checkpoint to /tmp/finetuned_model\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3478] 2024-07-10 16:19:41,667 >> Saving model checkpoint to /tmp/finetuned_model\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:19:41,744 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:733] 2024-07-10 16:19:41,744 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:19:41,745 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|configuration_utils.py:800] 2024-07-10 16:19:41,745 >> Model config LlamaConfig {\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"architectures\": [\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"LlamaForCausalLM\"\n",
      "5k8gfq1mzx-algo-1-cuufd  |   ],\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"attention_dropout\": 0.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"bos_token_id\": 128000,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"eos_token_id\": 128001,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_act\": \"silu\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"hidden_size\": 4096,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"initializer_range\": 0.02,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"intermediate_size\": 14336,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"max_position_embeddings\": 8192,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"mlp_bias\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"model_type\": \"llama\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_attention_heads\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_hidden_layers\": 32,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"num_key_value_heads\": 8,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"pretraining_tp\": 1,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"quantization_config\": {\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"bits\": 4,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"group_size\": 128,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"modules_to_not_convert\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"quant_method\": \"awq\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"version\": \"gemm\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |     \"zero_point\": true\n",
      "5k8gfq1mzx-algo-1-cuufd  |   },\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rms_norm_eps\": 1e-05,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_scaling\": null,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"rope_theta\": 500000.0,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"tie_word_embeddings\": false,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"torch_dtype\": \"float16\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"transformers_version\": \"4.42.3\",\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"use_cache\": true,\n",
      "5k8gfq1mzx-algo-1-cuufd  |   \"vocab_size\": 128256\n",
      "5k8gfq1mzx-algo-1-cuufd  | }\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2574] 2024-07-10 16:19:41,841 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2574] 2024-07-10 16:19:41,841 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2583] 2024-07-10 16:19:41,842 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|tokenization_utils_base.py:2583] 2024-07-10 16:19:41,842 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | ***** train metrics *****\n",
      "5k8gfq1mzx-algo-1-cuufd  | epoch                    =     4.9042\n",
      "5k8gfq1mzx-algo-1-cuufd  |   total_flos               =   596135GF\n",
      "5k8gfq1mzx-algo-1-cuufd  |   train_loss               =     1.2158\n",
      "5k8gfq1mzx-algo-1-cuufd  |   train_runtime            = 0:15:39.55\n",
      "5k8gfq1mzx-algo-1-cuufd  |   train_samples_per_second =      1.389\n",
      "5k8gfq1mzx-algo-1-cuufd  |   train_steps_per_second   =       0.17\n",
      "5k8gfq1mzx-algo-1-cuufd  | Figure saved at: /tmp/finetuned_model/training_loss.png\n",
      "5k8gfq1mzx-algo-1-cuufd  | 07/10/2024 16:19:42 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3788] 2024-07-10 16:19:42,122 >> \n",
      "5k8gfq1mzx-algo-1-cuufd  | ***** Running Evaluation *****\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3788] 2024-07-10 16:19:42,122 >> \n",
      "5k8gfq1mzx-algo-1-cuufd  | ***** Running Evaluation *****\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3790] 2024-07-10 16:19:42,122 >>   Num examples = 30\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3793] 2024-07-10 16:19:42,122 >>   Batch size = 1\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3790] 2024-07-10 16:19:42,122 >>   Num examples = 30\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|trainer.py:3793] 2024-07-10 16:19:42,122 >>   Batch size = 1\n",
      "5k8gfq1mzx-algo-1-cuufd  | 0%|          | 0/30 [00:00<?, ?it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 7%|▋         | 2/30 [00:00<00:01, 18.93it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 13%|█▎        | 4/30 [00:00<00:02, 11.30it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 20%|██        | 6/30 [00:00<00:02, 10.64it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 27%|██▋       | 8/30 [00:00<00:02, 10.96it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 33%|███▎      | 10/30 [00:00<00:01, 11.21it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 40%|████      | 12/30 [00:01<00:01, 10.73it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 47%|████▋     | 14/30 [00:01<00:01,  9.35it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 53%|█████▎    | 16/30 [00:01<00:01,  9.82it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 60%|██████    | 18/30 [00:01<00:01,  9.84it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 67%|██████▋   | 20/30 [00:01<00:01,  9.92it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 73%|███████▎  | 22/30 [00:02<00:00,  9.49it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 77%|███████▋  | 23/30 [00:02<00:00,  9.36it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 83%|████████▎ | 25/30 [00:02<00:00,  9.83it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 90%|█████████ | 27/30 [00:02<00:00, 10.40it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 97%|█████████▋| 29/30 [00:02<00:00, 10.84it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | 100%|██████████| 30/30 [00:02<00:00, 10.26it/s]\n",
      "5k8gfq1mzx-algo-1-cuufd  | ***** eval metrics *****\n",
      "5k8gfq1mzx-algo-1-cuufd  |   epoch                   =     4.9042\n",
      "5k8gfq1mzx-algo-1-cuufd  |   eval_loss               =     1.5544\n",
      "5k8gfq1mzx-algo-1-cuufd  |   eval_runtime            = 0:00:03.04\n",
      "5k8gfq1mzx-algo-1-cuufd  |   eval_samples_per_second =      9.845\n",
      "5k8gfq1mzx-algo-1-cuufd  |   eval_steps_per_second   =      9.845\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modelcard.py:449] 2024-07-10 16:19:45,172 >> Dropping the following result as it does not have all the necessary fields:\n",
      "5k8gfq1mzx-algo-1-cuufd  | {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "5k8gfq1mzx-algo-1-cuufd  | [INFO|modelcard.py:449] 2024-07-10 16:19:45,172 >> Dropping the following result as it does not have all the necessary fields:\n",
      "5k8gfq1mzx-algo-1-cuufd  | {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "5k8gfq1mzx-algo-1-cuufd  | *****************finished training, start cp finetuned model*****************************\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/README.md s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/README.md\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/trainer_state.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/trainer_state.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/runs/Jul10_16-03-29_algo-1-cuufd/events.out.tfevents.1720627442.algo-1-cuufd.227.0 s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/runs/Jul10_16-03-29_algo-1-cuufd/events.out.tfevents.1720627442.algo-1-cuufd.227.0\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/scheduler.pt s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/scheduler.pt\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/all_results.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/all_results.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/special_tokens_map.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/adapter_config.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/adapter_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/trainer_state.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/trainer_state.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/eval_results.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/eval_results.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/train_results.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/train_results.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/README.md s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/README.md\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/trainer_log.jsonl s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/trainer_log.jsonl\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/special_tokens_map.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/special_tokens_map.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/tokenizer_config.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/training_args.bin s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/training_args.bin\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/runs/Jul10_16-03-29_algo-1-cuufd/events.out.tfevents.1720628385.algo-1-cuufd.227.1 s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/runs/Jul10_16-03-29_algo-1-cuufd/events.out.tfevents.1720628385.algo-1-cuufd.227.1\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/adapter_config.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/adapter_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/training_args.bin s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/training_args.bin\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/training_loss.png s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/training_loss.png\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/tokenizer_config.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/tokenizer_config.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/rng_state.pth s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/rng_state.pth\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/tokenizer.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/tokenizer.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/tokenizer.json s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/tokenizer.json\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/adapter_model.safetensors s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/adapter_model.safetensors\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/adapter_model.safetensors s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/adapter_model.safetensors\n",
      "5k8gfq1mzx-algo-1-cuufd  | cp /tmp/finetuned_model/checkpoint-160/optimizer.pt s3://sagemaker-us-east-1-342367142984/llama3-8b-qlora/finetuned_model/checkpoint-160/optimizer.pt\n",
      "5k8gfq1mzx-algo-1-cuufd  | -----finished cp-------\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:19:47,646 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:19:47,646 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "5k8gfq1mzx-algo-1-cuufd  | 2024-07-10 16:19:47,646 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:creating /tmp/tmp1_2cnjfj/artifacts/output/data\n",
      "INFO:root:copying /tmp/tmp1_2cnjfj/algo-1-cuufd/output/success -> /tmp/tmp1_2cnjfj/artifacts/output\n",
      "INFO:sagemaker.local.image:===== Job Complete =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5k8gfq1mzx-algo-1-cuufd exited with code 0\n",
      "Aborting on container exit...\n",
      " Container 5k8gfq1mzx-algo-1-cuufd  Stopping\n",
      " Container 5k8gfq1mzx-algo-1-cuufd  Stopped\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 至此部，本章节结束\n",
    "- 模型已经在本地训练完成，并上传至s3 位置在 : s3://{default_bucket}/llama3-8b-qlora/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下是可选步骤，直接在本地使用LLaMA-Factory cli进行训练\n",
    "### 本地运行LLaMA-Factory cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#切换工作目录到LLaMA-Factory\n",
    "os.chdir('LLaMA-Factory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/ec2-user/SageMaker/llm_finetune/LLaMA-Factory\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml): started\n",
      "  Building editable for llamafactory (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.7.2.dev0-0.editable-py3-none-any.whl size=18819 sha256=3cbaa1b62e626d21787b17927d017a1567e30e626d3d4d138a226c374e0b5ee4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-iuuq88kr/wheels/83/8d/97/dcc5e92eb79a90d3bb9183e169b5e4d80d8bffa22009f80366\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "Successfully installed llamafactory-0.7.2.dev0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#安装LLaMA-Factory\n",
    "os.system(\"pip install --no-deps -e .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers>=4.41.2 (from -r requirements.txt (line 1))\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.19.2)\n",
      "Collecting accelerate>=0.30.1 (from -r requirements.txt (line 3))\n",
      "  Using cached accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft>=0.11.1 (from -r requirements.txt (line 4))\n",
      "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl>=0.8.6 (from -r requirements.txt (line 5))\n",
      "  Downloading trl-0.9.4-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gradio>=4.0.0 (from -r requirements.txt (line 6))\n",
      "  Using cached gradio-4.33.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.12.0)\n",
      "Collecting einops (from -r requirements.txt (line 8))\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentencepiece (from -r requirements.txt (line 9))\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tiktoken (from -r requirements.txt (line 10))\n",
      "  Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (4.25.3)\n",
      "Collecting uvicorn (from -r requirements.txt (line 12))\n",
      "  Using cached uvicorn-0.30.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (2.6.4)\n",
      "Collecting fastapi (from -r requirements.txt (line 14))\n",
      "  Using cached fastapi-0.111.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting sse-starlette (from -r requirements.txt (line 15))\n",
      "  Using cached sse_starlette-2.1.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (3.8.3)\n",
      "Collecting fire (from -r requirements.txt (line 17))\n",
      "  Using cached fire-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (21.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (6.0.1)\n",
      "Collecting deepspeed (from -r requirements.txt (line 20))\n",
      "  Using cached deepspeed-0.14.2-py3-none-any.whl\n",
      "Collecting autoawq (from -r requirements.txt (line 21))\n",
      "  Using cached autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting metrics (from -r requirements.txt (line 22))\n",
      "  Using cached metrics-0.3.3.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bitsandbytes (from -r requirements.txt (line 23))\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting rouge-chinese (from -r requirements.txt (line 24))\n",
      "  Using cached rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting jieba (from -r requirements.txt (line 25))\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (1.26.4)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.41.2->-r requirements.txt (line 1))\n",
      "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers>=4.41.2->-r requirements.txt (line 1)) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets>=2.16.0->-r requirements.txt (line 2)) (3.9.5)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate>=0.30.1->-r requirements.txt (line 3)) (2.1.0)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "  Using cached tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached ffmpy-0.3.2-py3-none-any.whl\n",
      "Collecting gradio-client==0.17.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached gradio_client-0.17.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\n",
      "Collecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.2.0)\n",
      "Collecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.10.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.2.1)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.17.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 12)) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 13)) (2.16.3)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->-r requirements.txt (line 14))\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi->-r requirements.txt (line 14))\n",
      "  Using cached fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi->-r requirements.txt (line 14)) (5.9.0)\n",
      "Collecting email_validator>=2.0.0 (from fastapi->-r requirements.txt (line 14))\n",
      "  Using cached email_validator-2.1.1-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sse-starlette->-r requirements.txt (line 15)) (4.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.9.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fire->-r requirements.txt (line 17)) (2.4.0)\n",
      "Collecting hjson (from deepspeed->-r requirements.txt (line 20))\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed->-r requirements.txt (line 20))\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting py-cpuinfo (from deepspeed->-r requirements.txt (line 20))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pynvml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 20)) (11.5.0)\n",
      "Collecting zstandard (from autoawq->-r requirements.txt (line 21))\n",
      "  Using cached zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting autoawq-kernels (from autoawq->-r requirements.txt (line 21))\n",
      "  Using cached autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting Pygments==2.2.0 (from metrics->-r requirements.txt (line 22))\n",
      "  Using cached Pygments-2.2.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pathspec==0.5.5 (from metrics->-r requirements.txt (line 22))\n",
      "  Using cached pathspec-0.5.5.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pathlib2>=2.3.0 (from metrics->-r requirements.txt (line 22))\n",
      "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (4.21.1)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14))\n",
      "  Using cached dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: idna>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 14)) (3.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.0.4)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers>=4.41.2->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (3.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->-r requirements.txt (line 5))\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "  Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "  Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 14))\n",
      "  Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->-r requirements.txt (line 6)) (0.18.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.5.2-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "INFO: pip is still looking at multiple versions of rich to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached rich-13.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached rich-13.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.3.5-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.3.4-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.3.3-py3-none-any.whl.metadata (18 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached rich-13.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting commonmark<0.10.0,>=0.9.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached rich-13.0.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-13.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.5.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.5.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.4.4-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.4.3-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.4.2-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.4.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Using cached rich-12.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-12.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-12.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-12.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-11.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.4.4)\n",
      "  Using cached rich-11.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.6->-r requirements.txt (line 5))\n",
      "  Using cached tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Using cached rich-11.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.16.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.16.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.16.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.15.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.15.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.15.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.14.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.13.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Using cached rich-10.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting metrics (from -r requirements.txt (line 22))\n",
      "  Using cached metrics-0.3.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached metrics-0.3.1.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached metrics-0.3.0.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached metrics-0.2.8.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pathspec==0.5.3 (from metrics->-r requirements.txt (line 22))\n",
      "  Using cached pathspec-0.5.3.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting metrics (from -r requirements.txt (line 22))\n",
      "  Using cached metrics-0.2.7.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Using cached metrics-0.2.6-py3-none-any.whl\n",
      "Requirement already satisfied: Pygments>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from metrics->-r requirements.txt (line 22)) (2.17.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.30.1->-r requirements.txt (line 3)) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Using cached accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
      "Using cached peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "Downloading trl-0.9.4-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached gradio-4.33.0-py3-none-any.whl (12.3 MB)\n",
      "Using cached gradio_client-0.17.0-py3-none-any.whl (316 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "Using cached fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "Using cached sse_starlette-2.1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached autoawq-0.2.5-cp310-cp310-manylinux2014_x86_64.whl (84 kB)\n",
      "Using cached bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
      "Using cached rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "Using cached email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
      "Using cached fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Using cached orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Using cached python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
      "Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tyro-0.8.4-py3-none-any.whl (102 kB)\n",
      "Using cached autoawq_kernels-0.0.6-cp310-cp310-manylinux2014_x86_64.whl (33.4 MB)\n",
      "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Using cached dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Using cached httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Using cached uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Using cached watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: sentencepiece, pydub, py-cpuinfo, ninja, jieba, hjson, ffmpy, zstandard, websockets, uvloop, uvicorn, toolz, tomlkit, shtab, shellingham, semantic-version, safetensors, ruff, rouge-chinese, regex, python-multipart, python-dotenv, orjson, metrics, mdurl, httptools, fire, einops, docstring-parser, dnspython, aiofiles, watchfiles, tiktoken, starlette, markdown-it-py, email_validator, tokenizers, sse-starlette, rich, gradio-client, deepspeed, bitsandbytes, autoawq-kernels, accelerate, tyro, typer, transformers, altair, trl, peft, fastapi-cli, autoawq, fastapi, gradio\n",
      "Successfully installed accelerate-0.30.1 aiofiles-23.2.1 altair-5.3.0 autoawq-0.2.5 autoawq-kernels-0.0.6 bitsandbytes-0.43.1 deepspeed-0.14.2 dnspython-2.6.1 docstring-parser-0.16 einops-0.8.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.33.0 gradio-client-0.17.0 hjson-3.1.0 httptools-0.6.1 jieba-0.42.1 markdown-it-py-3.0.0 mdurl-0.1.2 metrics-0.2.6 ninja-1.11.1.1 orjson-3.10.3 peft-0.11.1 py-cpuinfo-9.0.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2024.5.15 rich-13.7.1 rouge-chinese-1.0.3 ruff-0.4.8 safetensors-0.4.3 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 shtab-1.7.1 sse-starlette-2.1.0 starlette-0.37.2 tiktoken-0.7.0 tokenizers-0.19.1 tomlkit-0.12.0 toolz-0.12.1 transformers-4.41.2 trl-0.9.4 typer-0.12.3 tyro-0.8.4 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 zstandard-0.22.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/identity_2.json data/identity_2.json\n",
      "cp s3://sagemaker-us-east-1-434444145045/dataset-for-training/train/ruozhiba.json data/ruozhiba.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#下载数据集\n",
    "os.system(\"chmod +x ./s5cmd\")\n",
    "os.system(\"./s5cmd sync {0} {1}\".format(training_input_path+'/*', 'data/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启动训练\n",
    "本次训练过程大概15分钟左右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-06 15:15:02,102] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-06 15:15:04,360 >> loading file tokenizer.json from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-06 15:15:04,360 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-06 15:15:04,360 >> loading file special_tokens_map.json from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-06 15:15:04,360 >> loading file tokenizer_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible\n",
      "06/06/2024 15:15:04 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:314] 2024-06-06 15:15:04,678 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2024 15:15:04 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "06/06/2024 15:15:04 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>\n",
      "06/06/2024 15:15:04 - INFO - llamafactory.data.loader - Loading dataset identity_2.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset (num_proc=16): 100%|██████████| 91/91 [00:00<00:00, 414.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2024 15:15:05 - INFO - llamafactory.data.loader - Loading dataset ruozhiba.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting format of dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 949.10 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 291/291 [00:03<00:00, 91.06 examples/s] \n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-06 15:15:09,646 >> loading configuration file config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-06 15:15:09,647 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"TechxGenus/Meta-Llama-3-8B-Instruct-AWQ\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"bits\": 4,\n",
      "    \"group_size\": 128,\n",
      "    \"modules_to_not_convert\": null,\n",
      "    \"quant_method\": \"awq\",\n",
      "    \"version\": \"gemm\",\n",
      "    \"zero_point\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3474] 2024-06-06 15:15:09,664 >> loading weights file model.safetensors from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-06 15:15:09,665 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-06-06 15:15:09,666 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:\n",
      "[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 13, 128009, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 480, 12501, 12501, 46, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! I am RiverBot, an AI assistant developed by GOGOGO. How can I assist you today?<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 11188, 24406, 11, 459, 15592, 18328, 8040, 555, 480, 12501, 12501, 46, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "labels:\n",
      "Hello! I am RiverBot, an AI assistant developed by GOGOGO. How can I assist you today?<|eot_id|>\n",
      "06/06/2024 15:15:09 - INFO - llamafactory.model.utils.quantization - Loading 4-bit AWQ-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s]\n",
      "[INFO|modeling_utils.py:4280] 2024-06-06 15:15:11,325 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-06 15:15:11,325 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TechxGenus/Meta-Llama-3-8B-Instruct-AWQ.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-06 15:15:11,347 >> loading configuration file generation_config.json from cache at /home/ec2-user/.cache/huggingface/hub/models--TechxGenus--Meta-Llama-3-8B-Instruct-AWQ/snapshots/129d90727841a07bcdb3173ed4165d1353b44386/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-06 15:15:11,347 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ]\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:641] 2024-06-06 15:15:11,474 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/06/2024 15:15:11 - INFO - llamafactory.model.utils.checkpointing - Gradient checkpointing enabled.\n",
      "06/06/2024 15:15:11 - INFO - llamafactory.model.utils.attention - Using torch SDPA for faster training and inference.\n",
      "06/06/2024 15:15:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "06/06/2024 15:15:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "06/06/2024 15:15:11 - INFO - llamafactory.model.loader - trainable params: 3407872 || all params: 1054347264 || trainable%: 0.3232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2078] 2024-06-06 15:15:11,667 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-06 15:15:11,667 >>   Num examples = 261\n",
      "[INFO|trainer.py:2080] 2024-06-06 15:15:11,667 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2081] 2024-06-06 15:15:11,668 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2084] 2024-06-06 15:15:11,668 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-06-06 15:15:11,668 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2086] 2024-06-06 15:15:11,668 >>   Total optimization steps = 160\n",
      "[INFO|trainer.py:2087] 2024-06-06 15:15:11,669 >>   Number of trainable parameters = 3,407,872\n",
      "  1%|          | 1/160 [00:06<17:22,  6.56s/it]Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ec2-user/SageMaker/llm_finetune/LLaMA-Factory/src/llamafactory/cli.py\", line 93, in main\n",
      "    run_exp()\n",
      "  File \"/home/ec2-user/SageMaker/llm_finetune/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 33, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/home/ec2-user/SageMaker/llm_finetune/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 73, in run_sft\n",
      "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py\", line 1885, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py\", line 2216, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/trainer.py\", line 3250, in training_step\n",
      "    self.accelerator.backward(loss)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2121, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  1%|          | 1/160 [00:10<26:44, 10.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICES=0\n",
    "os.system(f\"CUDA_VISIBLE_DEVICES={DEVICES} llamafactory-cli train {sg_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上传Lora模型文件至S3保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"./s5cmd sync {0} {1}\".format(save_dir, f's3://{default_bucket}/llama3-8b-qlora/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora model file saved s3://sagemaker-us-east-1-434444145045/llama3-8b-qlora/\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lora model file saved s3://{default_bucket}/llama3-8b-qlora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
